{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIOS articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 79 characters:\n",
    "# -----------------------------------------------------------------------------\n",
    "# 72 characters (docstrings or comments):\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghozzis\\AppData\\Local\\Continuum\\anaconda3\\envs\\eios_anomaly\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "alt.renderers.set_embed_options(theme='dark')\n",
    "# alt.data_transformers.enable('json')\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "from os.path import isfile\n",
    "from lxml import etree\n",
    "import json\n",
    "from langdetect import detect, detect_langs\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from textblob import TextBlob\n",
    "# e.g. in conda console: python -m textblob.download_corpora lite\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "                             balanced_accuracy_score)\n",
    "from imblearn.metrics import geometric_mean_score, make_index_balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Pipeline\n",
    "get_id_url = False\n",
    "clean_eios_id_url = False\n",
    "load_signals_label = False\n",
    "compare_signals_eios = False\n",
    "label_eios = False\n",
    "find_signals_to_remove = False\n",
    "explore_mismatch = False\n",
    "sample_eios = False\n",
    "tokenize_eios = False\n",
    "train_trigrams = False\n",
    "compute_tfidf = False\n",
    "compute_w2v = False\n",
    "find_empty_w2v = False\n",
    "topic_modeling = False\n",
    "sentiment_analysis = False\n",
    "plot_sentiment = False\n",
    "perform_tsne = False\n",
    "plot_tsne = False\n",
    "build_train_test_sets = False\n",
    "train_classification_models = False\n",
    "compute_scores = False\n",
    "plot_write_scores = True\n",
    "overview_ouput = True\n",
    "\n",
    "# Options\n",
    "load_w2v_from_bin = False\n",
    "\n",
    "output_dir = 'out'\n",
    "log_path = (output_dir + '/log/log-' + datetime.datetime.now().strftime('%Y%m%d%H%M%S') \n",
    "            + '-message.txt')\n",
    "warning_path = (output_dir + '/log/log-' + datetime.datetime.now().strftime('%Y%m%d%H%M%S') \n",
    "                + '-warning.txt')\n",
    "\n",
    "keep_boards = ['Priority Diseases Global', 'Priority Sources']\n",
    "eios_data_path = {'json':'../data/eios_articles/priority_boards_en_201711_201908/json',\n",
    "                  'xml':('../data/eios_articles/'\n",
    "                         + 'en_onlyfor Stephane 2018 July to August')}\n",
    "eios_data_type = 'json'\n",
    "eiosid_to_remove_manually = [42941746]\n",
    "filter_text_n_letters = 30 # articles have to have at least filter_text_n_letters\n",
    "                           # latin letters\n",
    "filter_word_length = 1 # tokens have to be strictly longer than filter_word_length\n",
    "read_eios_from_date = pd.Timestamp(2017, 11, 1, 0)\n",
    "read_eios_to_date = pd.Timestamp(2019, 8, 31, 23)\n",
    "nosignal_sample_frac = 0.1\n",
    "nosignal_sample_seed = 42\n",
    "\n",
    "ngram_range_tfidf = (1,1)\n",
    "limit_load_w2v = None # None to load all embeddings (~3M), 500000 considered OK\n",
    "\n",
    "vectorization_methods = ['tfidf', 'tfidf_dr', 'w2v']\n",
    "standardizing = ['no_st','stand']\n",
    "n_components_dim_reduction = 300\n",
    "n_topics = 20\n",
    "chunksize_topics = 10000\n",
    "upsampling_methods = ['no_us','duplicate','adasyn']\n",
    "classification_methods = ['complement_naive_bayes','logistic_regression',\n",
    "                          'random_forest','multilayer_perceptron','svm_rbf']\n",
    "max_iter_lr = 10000\n",
    "randomforest_n_estimators = 100\n",
    "max_iter_mlp = 500\n",
    "mlp_lsize = (100,)\n",
    "\n",
    "scores_list = ['accuracy','precision','recall','specificity','f1','mcc','ba',\n",
    "               'geom_mean','iba_gm']\n",
    "n_thresholds = 1000\n",
    "alpha_iba = 0.1\n",
    "recall_target = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic functions\n",
    "def file_name_date(from_date, to_date):\n",
    "    return (from_date.strftime('%Y%m%d%H') + '_' + to_date.strftime('%Y%m%d%H'))\n",
    "\n",
    "def write_log(log_p, message):\n",
    "    if log_p is not None:\n",
    "        with open(log_p,'a+') as log_file:    \n",
    "            log_file.write(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                           + ' ' + message + '\\n')\n",
    "    return None\n",
    "\n",
    "def promed_id(url):\n",
    "    if (url.split('/')[0] == 'promedmail.org') & (url != 'promedmail.org'):\n",
    "        promedid = url.split('/')[-1].split('.')[-1]\n",
    "    else:\n",
    "        promedid = None\n",
    "    return promedid\n",
    "\n",
    "def match_urls(url, url_list):\n",
    "    \n",
    "    ## DEBUG:\n",
    "    # url = 'promedmail.org'\n",
    "    # url = 'promedmail.org/direct.php?id=20180702.58823' # in EIOS\n",
    "    # url = 'promedmail.org/post/5773973' # in signal list\n",
    "    # url = 'promedmail.org/direct.php?id=20190817.6624680' # in signal list\n",
    "    # url = 'elevenmyanmar.com/local/14680'\n",
    "    # url = 'reuters.com/article/venezuela-malaria/venezuelans-suffer-as-malaria-outbreak-spreads-in-drug-short-nation-idUSL1N1NQ0KJ'\n",
    "    # url_list = unique_signals_urls\n",
    "    # url_list = eios_id_url_cleaned.url_pp\n",
    "    \n",
    "    is_match = int(url.lower() in [u.lower() for u in url_list])\n",
    "    \n",
    "    # Match by ProMED ID but ignore when only domain given\n",
    "    # (URLs can be promedmail.org/direct.php?id=xxxxxxxx.yyyyyyy or\n",
    "    # promedmail.org/post/yyyyyyy or \n",
    "    # promedmail.org/post/xxxxxxxx.yyyyyyy)\n",
    "    if url == 'promedmail.org':\n",
    "        is_match = 0\n",
    "    if promed_id(url) is not None:\n",
    "        is_match = int(promed_id(url) in \n",
    "                       [promed_id(u) for u in url_list \n",
    "                        if promed_id(u) is not None])\n",
    "    \n",
    "    # info.gov.hk\n",
    "    if url.split('/')[0]=='info.gov.hk':\n",
    "        is_match = int(url.split('?')[0].lower() \n",
    "                       in [u.split('?')[0].lower() for u in url_list])\n",
    "    \n",
    "    # foodnews.com\n",
    "    if url.split('/')[0]=='foodnews.com':\n",
    "        is_match = int('/'.join(url.split('/')[0:4]).lower() \n",
    "                       in ['/'.join(u.split('/')[0:4]).lower() for u in url_list])\n",
    "                \n",
    "    return is_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP functions\n",
    "def preprocess_text(text, removedots, codedigits, removenumbers, lowercase, removeaccent, lemm, stem, filter_w):\n",
    "    # N.B. stop words are always removed, in gensim they include numbers \n",
    "    # written in letters: \"one\", \"two\", \"twenty\", ...\n",
    "    \n",
    "    ## DEBUG:\n",
    "#     text = (\"Good muffins cost $3.88\\nin New York.\"\n",
    "#             + \"  Please buy me\\ntwo of them!\\n\\nThanks.\"\n",
    "#             + \" Well, he said: \\\"They're the best!\\\" ``I'm not so sure,´´ I replied.\")\n",
    "#     removedots, codedigits, removenumbers, lowercase, removeaccent, lemm, stem, filter_w = True, False, True, True, True, True, True, 1\n",
    "#     removedots, codedigits, removenumbers, lowercase, removeaccent, lemm, stem, filter_w = False, True, False, False, False, False, False, 1\n",
    "\n",
    "    sentence_list = []\n",
    "    if (text is not None):\n",
    "        sentence_tokens = (nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "                           .sentences_from_text(text))\n",
    "        for sentence in sentence_tokens:\n",
    "            tokenized_sentence = []\n",
    "            token_list = nltk.word_tokenize(sentence)\n",
    "            for token in token_list:\n",
    "                token = re.sub('[^A-zÀ-ÿ0-9\\.]','',token)\n",
    "                token = re.sub('[\\´\\`\\^]','',token)\n",
    "                # If there are no other characters than \".\", set token to \"\"\n",
    "                if len(re.sub('\\.', '', token)) == 0:\n",
    "                    token = ''\n",
    "                if ((len(token) > filter_w) & \n",
    "                    (token.lower() not in gensim.parsing.preprocessing.STOPWORDS)):\n",
    "                    if removedots:\n",
    "                        token = re.sub('\\.','',token)\n",
    "                    if codedigits:\n",
    "                        token = re.sub('[0-9]','#',token)\n",
    "                    if removenumbers:\n",
    "                        if len(re.sub('[0-9]', '', token)) == 0:\n",
    "                            token = ''\n",
    "                    if lowercase:\n",
    "                        token = token.lower()\n",
    "                    if removeaccent:\n",
    "                        token = gensim.utils.deaccent(token)\n",
    "                    if lemm:\n",
    "                        token = WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "                    if stem:\n",
    "                        token = SnowballStemmer('english').stem(token)\n",
    "                    if len(token) > filter_w:\n",
    "                        tokenized_sentence.append(token)\n",
    "            sentence_list.append(tokenized_sentence)    \n",
    "    return sentence_list\n",
    "\n",
    "def word_embeddings_mean(wv, token_list):\n",
    "    # N.B. mean weighted by tf-idf might be better than simple mean, see:\n",
    "    # https://doi.org/10.1016/j.patrec.2016.06.012\n",
    "    # http://nadbordrozd.github.io/blog/2016/05/20/...\n",
    "    #...text-classification-with-word2vec/\n",
    "    \n",
    "    # If no embeddings are found, e.g. because the text is not in \n",
    "    # English, returns None\n",
    "    \n",
    "    ## DEBUG:\n",
    "#     wv = w2v\n",
    "#     token_list = ['Hi', 'there', 'flying', 'Paris', 'New_York', 'muffins', 'U.N.']\n",
    "#     token_list = text_list_simple_pp[0]\n",
    "    embeddings = []\n",
    "    for tok in token_list:\n",
    "        if tok in wv.vocab:\n",
    "            embeddings.append(wv.vectors_norm[wv.vocab[tok].index])\n",
    "    if len(embeddings) == 0:\n",
    "        mean_embedding = None\n",
    "    else:    \n",
    "        mean_embedding = np.array(embeddings, dtype='float32').mean(axis=0)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EIOS functions\n",
    "def load_process_eios(eios_file, content_type, content_sample, boards_present,\n",
    "                      filter_w, filter_n_l, log_p):\n",
    "\n",
    "    # Loads and processes the desired content from the given EIOS\n",
    "    # file, either XML or JSON\n",
    "\n",
    "    ## DEBUG:\n",
    "#     eios_file = ('../data/eios_articles/'\n",
    "#                  + 'en_onlyfor Stephane 2018 July to August/201807/'\n",
    "#                  + 'Finder_2018-07-15 02_00_00 UTC_2018-07-15 '\n",
    "#                  + '02_59_59 UTC_en.xml')\n",
    "#     eios_file = ('../data/eios_articles/priority_boards_en_201711_201908/'\n",
    "#                  + 'json/2018-07/'\n",
    "#                  + 'priority_boards_en_2018-07-08.json')\n",
    "#     content_sample = None\n",
    "# #     content_sample = sample_eios_labels\n",
    "#     content_type = ['id', 'url_pp', 'date']\n",
    "#     boards_present = keep_boards\n",
    "#     filter_w = 1\n",
    "#     filter_n_l = filter_text_n_letters\n",
    "#     log_p = warning_path\n",
    "#     # Example of eiosID with signal: 39315172\n",
    "\n",
    "    # For URLs: perform the same operations as for URLs in signals\n",
    "    # list: remove strings used there for splitting as well as\n",
    "    # leading and trailing characters (special or punctuation).\n",
    "    noninfo_substr = ['http://','https://','www.','www2.','wwwnc.']\n",
    "    strip_lead_trail = ' /,;.?!<>\\r\\n'\n",
    "\n",
    "    content_df = pd.DataFrame(columns = content_type)\n",
    "\n",
    "    file_format = eios_file.split('.')[-1]\n",
    "    if file_format == 'json':\n",
    "        known_content = {'id':'eiosId', 'title':'title', 'url_pp':'originalUrl', \n",
    "                         'url_full':'originalUrl', 'tokens_simple_pp':'fullText',\n",
    "                         'tokens_full_pp':'fullText', 'date':'fetchDate'}\n",
    "        if any([ct not in known_content.keys() for ct in content_type]):\n",
    "            raise ValueError(\"load_process_eios: I don't know how to extract \"\n",
    "                             + ', '.join(content_type) + ', I only know '\n",
    "                             + ', '.join(known_content.keys()) + '.')\n",
    "        with open(eios_file, encoding='utf8') as ef:\n",
    "            file_content = json.load(ef)\n",
    "        \n",
    "        # If the sample of EIOS articles isn't given, remove articles...\n",
    "        remove_article = ['no' for i in range(len(file_content))]\n",
    "        if content_sample is None:\n",
    "            for j in range(len(file_content)):\n",
    "                warning_text = ('load_process_eios: In file ' + eios_file + ':\\n'\n",
    "                                + 'Removing the article with index ' + str(j)\n",
    "                                + ' (eiosId: ' + str(file_content[j]['eiosId'])\n",
    "                                + ', date: ' + file_content[j]['fetchDate'] + ')')\n",
    "                # ... that don't have full text or at least some latin letters\n",
    "                if len(re.sub('[^A-zÀ-ÿ]', '', file_content[j]['fullText'])) < filter_n_l:\n",
    "                    write_log(log_p, warning_text + ' as it has no full text content'\n",
    "                              + ' or less than ' + str(filter_n_l)\n",
    "                              + ' latin letters!')\n",
    "                    remove_article[j] = 'alphabet'\n",
    "                # ... that don't belong to at least one of the desired boards\n",
    "                if (remove_article[j] == 'no') & (boards_present is not None):\n",
    "                    boards = [file_content[j]['relatedBoards'][i]['title']\n",
    "                              for i in range(len(file_content[j]['relatedBoards']))]\n",
    "                    if not any(kb in boards for kb in boards_present):\n",
    "                        write_log(log_p, warning_text \n",
    "                                  + \" as it doesn't belong to any of \" \n",
    "                                  + 'the desired boards!')\n",
    "                        remove_article[j] = 'boards'\n",
    "                # ... that are not in English\n",
    "                if remove_article[j] == 'no':\n",
    "                    try:\n",
    "                        language = detect(re.sub('[^A-zÀ-ÿ0-9\\ \\.\\;\\,\\!\\?]', '',\n",
    "                                          file_content[j]['fullText']))\n",
    "                    except Exception as e:\n",
    "                        write_log(log_p, warning_text + ': langdetect: '\n",
    "                                  + str(e))\n",
    "                        language = 'unknown'\n",
    "                    \n",
    "                    if language != 'en':\n",
    "                        write_log(log_p, warning_text  \n",
    "                                  + ' as it is apparently not in English!')\n",
    "                        remove_article[j] = 'language'\n",
    "        \n",
    "        # Process and save content\n",
    "        for iart in range(len(file_content)):\n",
    "            process_article = False\n",
    "            if content_sample is None:\n",
    "                process_article = True\n",
    "            else:\n",
    "                article_signal = content_sample.signal.loc[\n",
    "                    content_sample.id==file_content[iart][known_content['id']]\n",
    "                ]\n",
    "                if article_signal.shape[0] == 1:\n",
    "                    process_article = True\n",
    "                elif article_signal.shape[0] > 1:\n",
    "                    raise ValueError(warnin_text\n",
    "                                     + ' More than one article found with the article ID!')\n",
    "                    \n",
    "            if process_article:\n",
    "                tmp_content = {}\n",
    "                for ct in content_type:\n",
    "                    content = file_content[iart][known_content[ct]]\n",
    "                    if ct == 'url_pp':\n",
    "                        content = re.sub('|'.join(noninfo_substr), '', content)\n",
    "                        content = content.strip().strip(strip_lead_trail).strip()\n",
    "                    elif ct == 'tokens_simple_pp':\n",
    "                        content = preprocess_text(content, removedots=False,\n",
    "                                                  codedigits=True,\n",
    "                                                  removenumbers=False,\n",
    "                                                  lowercase=False, \n",
    "                                                  removeaccent=False,\n",
    "                                                  lemm=False, stem=False, \n",
    "                                                  filter_w=filter_w)\n",
    "                    elif ct == 'tokens_full_pp':\n",
    "                        content = preprocess_text(content, removedots=True,\n",
    "                                                  codedigits=False,\n",
    "                                                  removenumbers=True,\n",
    "                                                  lowercase=True, \n",
    "                                                  removeaccent=True,\n",
    "                                                  lemm=True, stem=True, \n",
    "                                                  filter_w=filter_w)          \n",
    "                    elif ct == 'date':\n",
    "                        content = pd.Timestamp(content).tz_convert('UTC')\n",
    "                    tmp_content[ct] = content\n",
    "                if content_sample is None:\n",
    "                    tmp_content['remove'] = remove_article[iart]\n",
    "                else:\n",
    "                    tmp_content['signal'] = int(article_signal)\n",
    "                content_df = content_df.append(tmp_content, ignore_index=True)\n",
    "\n",
    "    elif file_format == 'xml':\n",
    "        # N.B. The attribute prefix \"emm:\" is defined in the XML file as\n",
    "        # \"{http://emm.jrc.it}\" via 'xmlns:emm=\"http://emm.jrc.it\"' and we\n",
    "        # have to explicitly replace it (or at least I didn't how to do it\n",
    "        # automatically).\n",
    "        known_content = {'url_pp':'link', 'tokens_simple_pp':'{http://emm.jrc.it}text',\n",
    "                         'tokens_full_pp':'{http://emm.jrc.it}text', 'date':'pubDate'}\n",
    "        if any([ct not in known_content.keys() for ct in content_type]):\n",
    "            raise ValueError(\"load_process_eios: I don't know how to extract \"\n",
    "                             + ', '.join(content_type) + ', I only know '\n",
    "                             + ', '.join(known_content.keys()) + '.')\n",
    "\n",
    "        # Read EIOS data\n",
    "        tree = etree.parse(eios_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for item in root[0].iter('item'):\n",
    "            tmp_content = {}\n",
    "            for ct in content_type:\n",
    "                for co in item.iter(known_content[ct]):\n",
    "                    # There should be only one URL, text, date, etc., for\n",
    "                    # each article, but I couldn't find them without\n",
    "                    # iterating on item.\n",
    "                    if ct == 'url_pp':\n",
    "                        content = re.sub('|'.join(noninfo_substr), '', co.text)\n",
    "                        content = (content.strip().strip(strip_lead_trail)\n",
    "                                   .strip())\n",
    "                    elif ct == 'tokens_simple_pp':\n",
    "                        content = preprocess_text(co.text, removedots=False,\n",
    "                                                  codedigits=True,\n",
    "                                                  removenumbers=False,\n",
    "                                                  lowercase=False,\n",
    "                                                  removeaccent=False,\n",
    "                                                  lemm=False, stem=False, \n",
    "                                                  filter_w=filter_w)\n",
    "                    elif ct == 'tokens_full_pp':\n",
    "                        content = preprocess_text(co.text, removedots=True,\n",
    "                                                  codedigits=False,\n",
    "                                                  removenumbers=True,\n",
    "                                                  lowercase=True,\n",
    "                                                  removeaccent=True,\n",
    "                                                  lemm=True, stem=True, \n",
    "                                                  filter_w=filter_w)\n",
    "                    elif ct == 'date':\n",
    "                        content = pd.Timestamp(co.text).tz_convert('UTC')\n",
    "                    else:\n",
    "                        content = co.text\n",
    "                tmp_content[ct] = content\n",
    "                content_df = content_df.append(tmp_content, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('load_process_eios: '\n",
    "                         + \"I don't know how to deal with EIOS file format: \"\n",
    "                         + file_format + '.')\n",
    "\n",
    "    return content_df\n",
    "\n",
    "def get_eios(content_type, content_sample, data_source_type, time_from, time_to,\n",
    "             data_path, boards_present, filter_w, filter_n_l, verbose, log_p):\n",
    "\n",
    "    # Looks for EIOS files, assuming each corresponds to one\n",
    "    # hour of articles, from 0min 0s of time_from to 59min 59s of\n",
    "    # time_to (included).\n",
    "    # It can deal with two source types: data_source_type = 'json' or\n",
    "    # data_source_type = 'xml', which actually contain different information.\n",
    "    # Calls load_process_eios() on each file to load and process the\n",
    "    # desired content.\n",
    "\n",
    "    ## DEBUG:\n",
    "#     content_type = ['url_pp', 'date']\n",
    "#     content_sample = None\n",
    "#     content_sample = sample_eios_labels\n",
    "#     data_source_type = 'json'\n",
    "#     time_from = pd.Timestamp(2018, 7, 12, 0)\n",
    "#     time_to = pd.Timestamp(2019, 7, 12, 23)\n",
    "#     data_path = eios_data_path\n",
    "#     filter_w = 3\n",
    "#     filter_n_l = filter_text_n_letters\n",
    "#     verbose = True\n",
    "#     log_p = warning_path\n",
    "\n",
    "    file_path = data_path[data_source_type]\n",
    "    file_list = []\n",
    "    if data_source_type == 'json':\n",
    "        for timestamp in pd.date_range(time_from, time_to, freq = 'D'):\n",
    "            day_string = (str(timestamp.year) + '-' + '%02d' % timestamp.month\n",
    "                          + '-' + '%02d' % timestamp.day)\n",
    "            file_name = (file_path + '/' + str(timestamp.year) + '-'\n",
    "                         + '%02d' % timestamp.month + '/'\n",
    "                         + 'priority_boards_en_' + day_string + '.json')\n",
    "            if isfile(file_name):\n",
    "                file_list.append(file_name)\n",
    "            else:\n",
    "                write_log(log_p, 'get_eios: EIOS file ' \n",
    "                          + file_name + ' not found!')\n",
    "\n",
    "    elif data_source_type == 'xml':\n",
    "        for timestamp in pd.date_range(time_from, time_to, freq = 'H'):\n",
    "            day_string = (str(timestamp.year) + '-' + '%02d' % timestamp.month\n",
    "                          + '-' + '%02d' % timestamp.day)\n",
    "            file_name = (file_path + '/' + str(timestamp.year)\n",
    "                         + '%02d' % timestamp.month + '/' + 'Finder_'\n",
    "                         + day_string + ' ' + '%02d' % timestamp.hour\n",
    "                         + '_00_00 UTC_' + day_string + ' '\n",
    "                         + '%02d' % timestamp.hour + '_59_59 UTC_en.xml')\n",
    "            if isfile(file_name):\n",
    "                file_list.append(file_name)\n",
    "            else:\n",
    "                write_log(log_p, 'get_eios: EIOS file ' + file_name + ' not found!')\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"get_eios: I don't know hopw to find data of type \"\n",
    "                         + data_source_type + '!')\n",
    "\n",
    "    eios_data = pd.DataFrame(columns = content_type)\n",
    "\n",
    "    if verbose:\n",
    "        print('Find content:',\n",
    "              datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "              end = ' ')\n",
    "        ifile = 0\n",
    "    for file in file_list:\n",
    "        if verbose:\n",
    "            print(ifile, end = ' ')\n",
    "            ifile += 1\n",
    "        eios_data = eios_data.append(\n",
    "            load_process_eios(file, content_type, content_sample, boards_present,\n",
    "                              filter_w, filter_n_l, log_p),\n",
    "            ignore_index=True)\n",
    "    if verbose:\n",
    "        print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    return eios_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "if not os.path.exists('pickles'):\n",
    "    os.makedirs('pickles')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(output_dir + '/sentiment'):\n",
    "    os.makedirs(output_dir + '/sentiment')\n",
    "if not os.path.exists(output_dir + '/tsne'):\n",
    "    os.makedirs(output_dir + '/tsne')    \n",
    "    \n",
    "eios_id_url_file = ('pickles/eios_id_url-' \n",
    "                    + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                    + '.pickle')\n",
    "eios_id_url_cleaned_file = ('pickles/eios_id_url_cleaned-' \n",
    "                            + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                            + '.pickle')\n",
    "signals_data_file = 'pickles/signals_data.pickle'\n",
    "ebola_alerts_data_file = 'pickles/ebola_alerts_data.pickle'\n",
    "signals_media_links_file = 'pickles/signals_media_links.pickle'\n",
    "signals_domains_no_url_in_eios_file = (\n",
    "    'pickles/signals_domains_no_url_in_eios-' \n",
    "    + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "    + '.pickle'\n",
    ") \n",
    "eios_labels_file = ('pickles/eios_labels-' \n",
    "                    + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                    + '.pickle')                                     \n",
    "sample_eios_labels_file = ('pickles/sample_eios_labels-' + str(nosignal_sample_frac) \n",
    "                           + '_' + str(nosignal_sample_seed) + '-' \n",
    "                           + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                           + '.pickle')\n",
    "eios_tokens_file = ('pickles/eios_tokens-' + str(nosignal_sample_frac) \n",
    "                    + '_' + str(nosignal_sample_seed) + '-' \n",
    "                    + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                    + '.pickle')\n",
    "bigram_simple_pp_file = ('pickles/bigram_simple_pp-' + str(nosignal_sample_frac) \n",
    "                         + '_' + str(nosignal_sample_seed) + '-' \n",
    "                         + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                         + '.pickle')\n",
    "trigram_simple_pp_file = ('pickles/trigram_simple_pp-' + str(nosignal_sample_frac) \n",
    "                         + '_' + str(nosignal_sample_seed) + '-' \n",
    "                         + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                         + '.pickle')\n",
    "bigram_full_pp_file = ('pickles/bigram_full_pp-' + str(nosignal_sample_frac) \n",
    "                       + '_' + str(nosignal_sample_seed) + '-' \n",
    "                       + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                       + '.pickle')\n",
    "trigram_full_pp_file = ('pickles/trigram_full_pp-' + str(nosignal_sample_frac) \n",
    "                       + '_' + str(nosignal_sample_seed) + '-' \n",
    "                       + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                       + '.pickle')\n",
    "eios_tfidf_file = ('pickles/eios_tfidf-' + str(nosignal_sample_frac) \n",
    "                   + '_' + str(nosignal_sample_seed) + '-' \n",
    "                   + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                   + '.pickle')\n",
    "eios_tfidf_dictionary_file = ('pickles/eios_tfidf_dictionary-' \n",
    "                              + str(nosignal_sample_frac) \n",
    "                              + '_' + str(nosignal_sample_seed) + '-' \n",
    "                              + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                              + '.pickle')\n",
    "w2v_pickle_file = 'pickles/w2v_' + str(limit_load_w2v) + '.pickle'\n",
    "eios_w2v_file = ('pickles/eios_w2v-' + str(limit_load_w2v) + '-'\n",
    "                 + str(nosignal_sample_frac) + '_' \n",
    "                 + str(nosignal_sample_seed) + '-' \n",
    "                 + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                 + '.pickle')\n",
    "w2v_examples_file = (output_dir + '/w2v_examples-' + str(limit_load_w2v) + '.txt')\n",
    "topics_file = ('pickles/topics-' \n",
    "               + str(nosignal_sample_frac) \n",
    "               + '_' + str(nosignal_sample_seed) + '-' \n",
    "               + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "               + '.pickle')\n",
    "topics_text_file = (output_dir + '/topics-'\n",
    "                    + str(nosignal_sample_frac) \n",
    "                    + '_' + str(nosignal_sample_seed) + '-' \n",
    "                    + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                    + '.txt')\n",
    "sentiment_file = ('pickles/sentiment-' \n",
    "               + str(nosignal_sample_frac) \n",
    "               + '_' + str(nosignal_sample_seed) + '-' \n",
    "               + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "               + '.pickle')\n",
    "sentiment_plot_file = (output_dir + '/sentiment/sentiment_plot-'\n",
    "                       + str(nosignal_sample_frac) \n",
    "                       + '_' + str(nosignal_sample_seed) + '-' \n",
    "                       + file_name_date(read_eios_from_date, read_eios_to_date))                       \n",
    "tsne_results_file = ('pickles/tsne_results-' + str(nosignal_sample_frac) \n",
    "                     + '_' + str(nosignal_sample_seed) + '-'\n",
    "                     + str(limit_load_w2v) + '-'\n",
    "                     + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                     + '.pickle')\n",
    "vec_tag = {'tfidf':'tfidf', 'tfidf_dr':'tfidf_dr',\n",
    "           'w2v': 'w2v_' + str(limit_load_w2v)}\n",
    "tsne_plot_files = {}\n",
    "for vc_met in ['tfidf','w2v']:\n",
    "    tsne_plot_files[vc_met] = (output_dir + '/tsne/tsne_signal_plot-'\n",
    "                               + str(nosignal_sample_frac) \n",
    "                               + '_' + str(nosignal_sample_seed) + '-'\n",
    "                               + vec_tag[vc_met] + '-'\n",
    "                               + file_name_date(read_eios_from_date, read_eios_to_date))\n",
    "trainset_file = ('pickles/trainset-' + str(nosignal_sample_frac) \n",
    "                 + '_' + str(nosignal_sample_seed) + '-' \n",
    "                 + str(limit_load_w2v) + '-'\n",
    "                 + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                 + '.pickle')\n",
    "testset_file = ('pickles/testset-' + str(nosignal_sample_frac) \n",
    "                + '_' + str(nosignal_sample_seed) + '-'\n",
    "                + str(limit_load_w2v) + '-'\n",
    "                + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                + '.pickle')\n",
    "trained_models_file = ('pickles/trained_models-' + str(nosignal_sample_frac) \n",
    "                       + '_' + str(nosignal_sample_seed) + '-'\n",
    "                       + str(limit_load_w2v) + '-'\n",
    "                       + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "                       + '.pickle')\n",
    "scores_file = ('pickles/scores-' + str(nosignal_sample_frac) + '_'\n",
    "               + str(nosignal_sample_seed) + '-' + str(limit_load_w2v)\n",
    "               + '-' + file_name_date(read_eios_from_date, read_eios_to_date)\n",
    "               + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EIOS article IDs and URLs\n",
    "if get_id_url:\n",
    "    write_log(log_path, 'Start getting EIOS IDs, URLs and dates')\n",
    "    eios_id_url = get_eios(['id', 'url_pp', 'date'], None,\n",
    "                           eios_data_type, read_eios_from_date, read_eios_to_date, \n",
    "                           eios_data_path, keep_boards, filter_word_length, \n",
    "                           filter_text_n_letters, True, None)\n",
    "    write_log(log_path, 'Getting EIOS IDs, URLs and dates done')\n",
    "    pickle.dump(eios_id_url, open(eios_id_url_file, 'wb'))\n",
    "    del eios_id_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove duplicates, keeping the oldest one; remove articles manually\n",
    "if clean_eios_id_url:\n",
    "    if not 'eios_id_url' in globals():\n",
    "        eios_id_url = pickle.load(open(eios_id_url_file, 'rb'))\n",
    "    write_log(log_path, 'Number of EIOS URL duplicates = '\n",
    "              + str(len(eios_id_url.url_pp)-len(eios_id_url.url_pp.unique())))\n",
    "    eios_id_url_cleaned = eios_id_url.copy()\n",
    "    eios_id_url_cleaned['date_min'] = (eios_id_url_cleaned\n",
    "                                       .groupby('url_pp')\n",
    "                                       .date\n",
    "                                       .transform('min'))\n",
    "    eios_id_url_cleaned = eios_id_url_cleaned.drop(\n",
    "        eios_id_url_cleaned[eios_id_url_cleaned.date != eios_id_url_cleaned.date_min]\n",
    "        .index).reset_index(drop=True)\n",
    "    eios_id_url_cleaned = eios_id_url_cleaned.drop(columns='date_min')\n",
    "\n",
    "    write_log(log_path, 'Number of EIOS URL duplicates after cleaning by date = '\n",
    "              + str(len(eios_id_url_cleaned.url_pp)\n",
    "                    - len(eios_id_url_cleaned.url_pp.unique())))\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    idx_keep_articles = eios_id_url_cleaned[['url_pp','date']].drop_duplicates().index\n",
    "    eios_id_url_cleaned = eios_id_url_cleaned.loc[idx_keep_articles].reset_index(drop=True)\n",
    "    \n",
    "    # Manually remove some articles\n",
    "    eios_id_url_cleaned = (eios_id_url_cleaned.drop(\n",
    "        eios_id_url_cleaned[eios_id_url_cleaned.id.isin(eiosid_to_remove_manually)].index)\n",
    "                           .reset_index(drop=True))\n",
    "    \n",
    "    write_log(log_path, 'After cleaning EIOS URLs, of overall ' \n",
    "              + str(eios_id_url_cleaned.shape[0]) + ' articles, '\n",
    "              + str(sum([rem != 'no' for rem in eios_id_url_cleaned.remove])) \n",
    "              + ' were flagged for removal (' \n",
    "              + str(sum([rem == 'alphabet' for rem in eios_id_url_cleaned.remove])) \n",
    "              + ' because of alphabet, '\n",
    "              + str(sum([rem == 'boards' for rem in eios_id_url_cleaned.remove])) \n",
    "              + ' because of boards, ' \n",
    "              + str(sum([rem == 'language' for rem in eios_id_url_cleaned.remove])) \n",
    "              + ' because of language)')\n",
    "    pickle.dump(eios_id_url_cleaned, open(eios_id_url_cleaned_file, 'wb'))\n",
    "    del eios_id_url, eios_id_url_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signals and Ebola alerts, label if matches EIOS URL\n",
    "if load_signals_label:\n",
    "    if not 'eios_id_url_cleaned' in globals():\n",
    "        eios_id_url_cleaned = pickle.load(open(eios_id_url_file, 'rb'))\n",
    "\n",
    "    signals_data = pickle.load(open(signals_data_file, 'rb'))\n",
    "    ebola_alerts_data = pickle.load(open(ebola_alerts_data_file, 'rb'))\n",
    "    signals_media_links = (signals_data.append(ebola_alerts_data)\n",
    "                           .reset_index(drop=True))\n",
    "    signals_media_links = signals_media_links.drop_duplicates()\n",
    "    signals_media_links['in_eios'] = [match_urls(url, eios_id_url_cleaned.url_pp) \n",
    "                                      for url in signals_media_links.url]\n",
    "    display(signals_media_links.head())\n",
    "    pickle.dump(signals_media_links, open(signals_media_links_file, 'wb'))\n",
    "\n",
    "    # TODO: clean disease and country; by duplicates, keep oldest\n",
    "    # (signals_media_links.loc[signals_media_links.url.duplicated(keep=False)]\n",
    "    #  .sort_values(by='url'))\n",
    "    # np.sort(signals_media_links.disease.unique())\n",
    "    # np.sort(signals_media_links.country.unique())\n",
    "    unique_signals_urls = np.sort(signals_media_links.url.unique())\n",
    "    write_log(log_path, 'There are ' + str(len(unique_signals_urls))\n",
    "              + ' unique signal URLs for overall ' + str(signals_media_links.shape[0]) \n",
    "              + ' signals (before cleaning disease and country)')\n",
    "    del eios_id_url_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare URLs in signal list and in EIOS\n",
    "if compare_signals_eios:\n",
    "    if not 'eios_id_url_cleaned' in globals():\n",
    "        eios_id_url_cleaned = pickle.load(open(eios_id_url_cleaned_file, 'rb'))\n",
    "    if not 'signals_media_links' in globals():    \n",
    "        signals_media_links = pickle.load(open(signals_media_links_file, 'rb'))\n",
    "\n",
    "    signals_media_links_timerange = (\n",
    "        signals_media_links.loc[\n",
    "            (signals_media_links.date >= read_eios_from_date + datetime.timedelta(days=7)) \n",
    "            & (signals_media_links.date <= read_eios_to_date),\n",
    "            :]\n",
    "    )\n",
    "    write_log(log_path, 'Number of signals between first date'\n",
    "              + ' plus 7 days and last date'\n",
    "              + ' (date is signal date) = '\n",
    "              + str(signals_media_links_timerange.shape[0]))\n",
    "    write_log(log_path, \n",
    "              'Of those, number that could *not* be matched to EIOS = '\n",
    "              + str(signals_media_links_timerange.shape[0]\n",
    "                    - sum(signals_media_links_timerange.in_eios)))\n",
    "\n",
    "    signals_domains_timerange = np.sort(np.unique(\n",
    "        [url.split('/')[0] for url in signals_media_links_timerange.url]\n",
    "    ))\n",
    "    eios_domains = np.sort(np.unique(\n",
    "        [url.split('/')[0] for url in eios_id_url_cleaned.url_pp]\n",
    "    ))\n",
    "    signals_domains_not_in_eios = signals_media_links_timerange.loc[\n",
    "        [signals_media_links_timerange.url.iloc[i].split('/')[0] not in eios_domains\n",
    "         for i in range(signals_media_links_timerange.shape[0])]\n",
    "    ]\n",
    "    signals_domains_no_url_in_eios = signals_media_links_timerange.loc[\n",
    "        [(signals_media_links_timerange.url.iloc[i].split('/')[0] in eios_domains)\n",
    "         & (signals_media_links_timerange.in_eios.iloc[i] == 0)\n",
    "         for i in range(signals_media_links_timerange.shape[0])]\n",
    "    ]\n",
    "    write_log(log_path,\n",
    "              'Number of signals which domain is *not* in EIOS = '\n",
    "              + str(signals_domains_not_in_eios.shape[0]))\n",
    "    write_log(log_path,\n",
    "              'Number of signals which domain *is* in EIOS'\n",
    "              + ' but URLs could *not* be matched = '\n",
    "              + str(signals_domains_no_url_in_eios.shape[0]))\n",
    "    write_log(log_path, 'Domains that have signals but are not in EIOS: '\n",
    "              + ', '.join(np.sort(np.unique(\n",
    "                  [url.split('/')[0] for url in signals_domains_not_in_eios.url]))))\n",
    "    display(signals_domains_no_url_in_eios.head()) \n",
    "    \n",
    "    pickle.dump(signals_domains_no_url_in_eios,\n",
    "                open(signals_domains_no_url_in_eios_file, 'wb'))\n",
    "    \n",
    "    del eios_id_url_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for articles: 1 if URL in signal list, 0 else\n",
    "if label_eios:\n",
    "    if not 'eios_id_url_cleaned' in globals():\n",
    "        eios_id_url_cleaned = pickle.load(open(eios_id_url_cleaned_file, 'rb'))\n",
    "    eios_labels = eios_id_url_cleaned.copy()\n",
    "    eios_labels['signal'] = [match_urls(url, unique_signals_urls) \n",
    "                             for url in eios_labels.url_pp]\n",
    "    display(eios_labels.head())\n",
    "    pickle.dump(eios_labels, open(eios_labels_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signals flagged for removal\n",
    "if find_signals_to_remove:\n",
    "    if not 'eios_labels' in globals():\n",
    "        eios_labels = pickle.load(open(eios_labels_file, 'rb'))    \n",
    "    eios_labels_signals_remove = (eios_labels.loc[(eios_labels.signal == 1)\n",
    "                                                 & (eios_labels.remove != 'no')]\n",
    "                                  .sort_values(by=['remove','url_pp']))\n",
    "    display(eios_labels_signals_remove)\n",
    "    write_log(log_path, 'From '\n",
    "              + str(sum(eios_labels.signal)) + ' matched signal URLs, '\n",
    "              + str(eios_labels_signals_remove.shape[0]) \n",
    "              + ' were flagged for removal (' \n",
    "              + str(sum([rem == 'alphabet' for rem in eios_labels_signals_remove.remove])) \n",
    "              + ' because of alphabet, '\n",
    "              + str(sum([rem == 'boards' for rem in eios_labels_signals_remove.remove])) \n",
    "              + ' because of boards, ' \n",
    "              + str(sum([rem == 'language' for rem in eios_labels_signals_remove.remove])) \n",
    "              + ' because of language)... But I\\'ll keep those not in the desired boards.')   \n",
    "    del eios_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore mismatches: Signal URLs not found in the EIOS dataset\n",
    "if explore_mismatch:\n",
    "    if not 'eios_labels' in globals():\n",
    "        eios_labels = pickle.load(open(eios_labels_file, 'rb'))\n",
    "    if not 'signals_domains_no_url_in_eios' in globals():\n",
    "        signals_domains_no_url_in_eios = pickle.load(open(signals_domains_no_url_in_eios_file, 'rb'))\n",
    "        \n",
    "    signals_domains_no_url_in_eios = signals_domains_no_url_in_eios.sort_values(by='url')\n",
    "    signals_domains_no_url_in_eios['domain'] = [url.split('/')[0] \n",
    "                                                for url in signals_domains_no_url_in_eios.url]\n",
    "    signals_domains_no_url_in_eios['subdirectory'] = [\n",
    "        '/'.join(url.split('/')[0:-1]) for url in signals_domains_no_url_in_eios.url\n",
    "    ]\n",
    "    domain_count = (signals_domains_no_url_in_eios.domain\n",
    "                    .value_counts(dropna=False).to_frame()) \n",
    "    eios_labels['domain'] = [url.split('/')[0] for url in eios_labels.url_pp]\n",
    "    eios_labels['subdirectory'] = [\n",
    "        '/'.join(url.split('/')[0:-1]) for url in eios_labels.url_pp\n",
    "    ]\n",
    "    eios_labels_subdirectories = np.sort(eios_labels.subdirectory.unique())\n",
    "\n",
    "    url_mismatch_log = 'Explore mismatches: Signal URLs not found in the EIOS dataset\\n\\n'\n",
    "    url_mismatch_log += ('='*80 + '\\n')\n",
    "    url_mismatch_log += ('Number of signal URLs not in EIOS: '\n",
    "                         + str(signals_domains_no_url_in_eios.shape[0]) + '\\n')\n",
    "    url_mismatch_log += ('Number of corresponding domains: ' + str(len(domain_count)) + '\\n')\n",
    "    url_mismatch_log += (', '.join([str(dom) + ': ' + str(domain_count.loc[dom].domain)\n",
    "                                    for dom in domain_count.index]) + '\\n')\n",
    "    url_mismatch_log += ('='*80 + '\\n')\n",
    "\n",
    "    # Inspect 10 selected domains (were those with the most signal URLs\n",
    "    # before improving the matching function)\n",
    "    depth_generic_subdirectory = {\n",
    "        'promedmail.org':1,'reliefweb.int':2,'moh.gov.sa':5,'g1.globo.com':2,\n",
    "        'paho.org':2,'reuters.com':2,'timesofindia.indiatimes.com':2,\n",
    "        'info.gov.hk':3,'angop.ao':5,'foodsafetynews.com':1\n",
    "    }\n",
    "    url_mismatch_log += ('Selected domains: '\n",
    "                          + ', '.join([k for k in depth_generic_subdirectory.keys()]) + '\\n')\n",
    "    url_mismatch_log += ('Number of signal URLs in those domains: '\n",
    "                         + str(sum([domain_count.loc[dom].domain\n",
    "                                    for dom in depth_generic_subdirectory.keys()]))\n",
    "                         + '\\n\\n')\n",
    "\n",
    "    manually_retained_urls = {\n",
    "        'promedmail.org':['promedmail.org/direct.php?id=20171107.5426321',\n",
    "                          'promedmail.org/direct.php?id=20171107.5428288',\n",
    "                          'promedmail.org/direct.php?id=20171107.5429761',\n",
    "                          'promedmail.org/direct.php?id=20171108.5429481',\n",
    "                          'promedmail.org/direct.php?id=20171109.5431669',\n",
    "                          'promedmail.org/direct.php?id=20180828.5989333',\n",
    "                          'promedmail.org/post/20171109.5433077',\n",
    "                          'promedmail.org/post/5828155',\n",
    "                          'promedmail.org/post/6312986'],\n",
    "        'reliefweb.int':['reliefweb.int/report/kenya/quarantine-tana-delta-after-rift-valley-fever-outbreak',\n",
    "                         'reliefweb.int/report/palau/dengue-3-outbreak-palau-december-2018-may-2019-report-date-may-28-2019',\n",
    "                         'reliefweb.int/report/sudan/acute-watery-diarrhoea-dengue-fever-reported-after-rains-sudan-s-blue-nile-red-sea',\n",
    "                         'reliefweb.int/report/sudan/red-sea-hospital-reports-new-watery-diarrhoea-cases'],\n",
    "        'moh.gov.sa':['None'],'g1.globo.com':['None'],\n",
    "        'paho.org':['None'],\n",
    "        'reuters.com':['reuters.com/article/us-france-babymilk-victims/lactalis-victims-group-says-10-more-babies-have-salmonella-idUSKBN1FF239',\n",
    "                       'reuters.com/article/us-health-birdflu-japan/japan-reports-first-suspected-bird-flu-case-in-poultry-this-winter-idUSKBN1EZ0R3?feedType=RSS&feedName=healthNews',\n",
    "                       'reuters.com/article/us-kenya-cholera/cholera-cases-rise-in-kenyas-capital-top-hospital-says-idUSKCN1RS0Y8',\n",
    "                       'reuters.com/article/us-mideast-crisis-syria-hama/syrian-state-media-says-rebels-shell-village-with-gas-injuring-21-idUSKCN1R40SB',\n",
    "                       'reuters.com/article/us-nigeria-health-cholera/cholera-outbreak-kills-12-in-northeast-nigeria-idUSKCN1IO11E',\n",
    "                       'reuters.com/article/us-uganda-congo-refugees/diarrhea-kills-26-congolese-refugees-in-uganda-infects-hundreds-u-n-idUSKCN1G61UI',\n",
    "                       'reuters.com/article/us-venezuela-malaria/venezuelans-suffer-as-malaria-outbreak-spreads-in-drug-short-nation-idUSKBN1DO1ES'],\n",
    "        'timesofindia.indiatimes.com':['timesofindia.indiatimes.com/city/bengaluru/three-nipah-cases-suspected-from-bengaluru/articleshow/64374584.cms',\n",
    "                                       'timesofindia.indiatimes.com/city/kozhikode/west-nile-fever-case-confirmed-in-malappuram/articleshow/68378544.cms',\n",
    "                                       'timesofindia.indiatimes.com/city/kozhikode/west-nile-fever-case-confirmed-in-malappuram/articleshow/68378544.cms',\n",
    "                                       'timesofindia.indiatimes.com/city/mumbai/eight-deaths-of-children-in-2-months-aes-suspected/articleshow/70388048.cms',\n",
    "                                       'timesofindia.indiatimes.com/city/puducherry/another-patient-suspected-of-nipah-infection-admitted-at-jipmer/articleshow/69849226.cms',\n",
    "                                       'timesofindia.indiatimes.com/city/puducherry/patient-with-suspected-nipah-infection-still-critical/articleshow/69763366.cms'],\n",
    "        'info.gov.hk':['info.gov.hk/gia/general/201801/16/P2018011600785.htm',\n",
    "                       'info.gov.hk/gia/general/201801/25/P2018012500863.htm?fontSize=1',\n",
    "                       'info.gov.hk/gia/general/201802/14/P2018021400759.htm',\n",
    "                       'info.gov.hk/gia/general/201907/22/P2019072200459.htm?fontSize=1'],\n",
    "        'angop.ao':['angop.ao/angola/en_us/noticias/sociedade/2018/3/15/Cuanza-Norte-with-dozens-suspected-cases-dengue-fever,7b19294e-388d-4f5b-a7c9-bb17e70ff466.html'],\n",
    "        'foodsafetynews.com':['foodsafetynews.com/2018/06/salmonella-outbreaks-from-raw-frozen-chicken-not-related/#.WxfJAO6FNhF',\n",
    "                              'foodsafetynews.com/2018/08/sweden-investigates-nationwide-e-coli-outbreak-source-unknown',\n",
    "                              'foodsafetynews.com/2018/10/a-dozen-cases-and-two-dead-in-listeria-outbreak-swiss-officials-suspect-food',\n",
    "                              'foodsafetynews.com/2018/12/denmark-probes-pork-link-in-salmonella-outbreak-strain-is-antibiotic-resistant',\n",
    "                              'foodsafetynews.com/2019/03/united-nations-food-aid-linked-to-3-deaths-262-poisonings-in-one-week']\n",
    "    }\n",
    "    for dom in depth_generic_subdirectory.keys():\n",
    "        url_mismatch_log += ('Number of signal URLs in ' + dom + ' = '\n",
    "                             + str(int(domain_count.loc[dom])) + '\\n')\n",
    "        sig_not_in_eios = signals_domains_no_url_in_eios.loc[\n",
    "            [domain==dom for domain in signals_domains_no_url_in_eios.domain]\n",
    "        ]\n",
    "        url_mismatch_log += ('    ' + '\\n    '.join(sig_not_in_eios.url) + '\\n')\n",
    "        subdir = ['/'.join(sb.split('/')[0:depth_generic_subdirectory[dom]]) \n",
    "                  for sb in sig_not_in_eios.subdirectory]\n",
    "        subdir_eios = ['/'.join(sb1.split('/')[0:depth_generic_subdirectory[dom]])\n",
    "                       for sb1 in eios_labels_subdirectories]\n",
    "        url_mismatch_log += ('\\nGeneric subdirectories have level '\n",
    "                             + str(depth_generic_subdirectory[dom]-1)\n",
    "                             + ':' + '\\n')\n",
    "        url_mismatch_log += ('    ' + '\\n    '.join(np.unique(subdir)) + '\\n\\n')\n",
    "        url_mismatch_log += ('Of those, found in the EIOS dataset:\\n')\n",
    "        url_mismatch_log += ('    '\n",
    "                             + '\\n    '.join(np.unique([sb for sb in subdir_eios if sb in subdir]))\n",
    "                             + '\\n\\n')\n",
    "        url_mismatch_log += (\n",
    "            'Number of URLs with corresponding subdirectory found in the EIOS dataset = '\n",
    "            + str(sum([sb in subdir_eios for sb in subdir]))\n",
    "            + '\\n'\n",
    "        )\n",
    "        url_mismatch_log += (\n",
    "            'After manual inspection, URLs that are valid (no 404 error, even'\n",
    "            + ' after removing manually possible tags, e.g.'\n",
    "            + ' \"?utm_medium=email&utm_source=user\"),'\n",
    "            + ' have the subdirectory in the EIOS dataset,'\n",
    "            + ' are in English and are not PDFs:\\n'\n",
    "        )\n",
    "        url_mismatch_log += ('    '\n",
    "                             + '\\n    '.join(manually_retained_urls[dom])\n",
    "                             + '\\n\\n')\n",
    "        if manually_retained_urls[dom] != ['None']:\n",
    "            url_mismatch_log += (\n",
    "                'These ' + str(len(manually_retained_urls[dom])) \n",
    "                + ' URLs were presumably not categorized in the desired boards or as English...?\\n'\n",
    "            )\n",
    "        if dom == 'reuters.com':\n",
    "            url_mismatch_log += (\n",
    "                'There are close matches for two:\\n'\n",
    "                + 'reuters.com/article/us-france-babymilk-victims/lactalis-victims-group-says-10-more-babies-have-salmonella-idUSKBN1FF239'\n",
    "                + ' has a similar article later in EIOS'\n",
    "                + ' (same subdir \"us-france-babymilk-victims\")\\n'\n",
    "                + 'reuters.com/article/us-venezuela-malaria/venezuelans-suffer-as-malaria-outbreak-spreads-in-drug-short-nation-idUSKBN1DO1ES'\n",
    "                + ' has a different subdirectory (\"venezuela-malaria\") and a different \"id\" in EIOS,'\n",
    "                + ' but the redirects to this one.\\n')\n",
    "            url_mismatch_log += (\n",
    "                'OK that the first doesn\\'t match, for the second one'\n",
    "                + ' one could write a Reuters-specific matching but not clear'\n",
    "                + ' it wouldn\\'t have side effects.\\n')\n",
    "        elif dom == 'paho.org':\n",
    "            url_mismatch_log += (\n",
    "                'All 4 are PDFs... but one could maybe parse'\n",
    "                + ' the URLs for \"ItemId\" and other tags and find the page in'\n",
    "                + ' EIOS that links to the PDF.\\n')\n",
    "        url_mismatch_log += ('='*80 + '\\n')\n",
    "\n",
    "    url_mismatch_log += ('Conclusion: After inspecting 10 selected domains:\\n')\n",
    "    url_mismatch_log += (\n",
    "        str(sum([len(manually_retained_urls[dom])\n",
    "                 for dom in manually_retained_urls.keys()\n",
    "                 if manually_retained_urls[dom] != ['None']]))\n",
    "        + ' (1 URL is counted twice) out of '\n",
    "        + str(sum([domain_count.loc[dom].domain\n",
    "                   for dom in depth_generic_subdirectory.keys()]))\n",
    "        + ' \"should\" have been found in the EIOS dataset, but'\n",
    "        + ' were presumably either not catagorized in the desired boards or as English.'\n",
    "        + ' One could maybe find a way to get the 4 paho.org'\n",
    "        + ' URLs by parsing for tags such as \"ItemId\". Other than that, the matching'\n",
    "        + ' function works well (no URL found that should have been matched).\\n\\n')\n",
    "    \n",
    "    write_log(log_path, url_mismatch_log)\n",
    "    del eios_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all articles that are signals, keep nosignal_sample_frac articles that are not signals\n",
    "if sample_eios:\n",
    "    if not 'eios_labels' in globals():\n",
    "        eios_labels = pickle.load(open(eios_labels_file, 'rb'))\n",
    "    eios_id_signal_keep = eios_labels.loc[(eios_labels.signal==1) \n",
    "                                          & (eios_labels.remove.isin(['no','boards']))]\n",
    "    eios_id_nosignal_keep = eios_labels.loc[(eios_labels.signal==0) \n",
    "                                            & (eios_labels.remove=='no')]\n",
    "    frac_eios_id_nosignal = eios_id_nosignal_keep.sample(frac=nosignal_sample_frac,\n",
    "                                                    random_state=nosignal_sample_seed)\n",
    "    sample_eios_labels = eios_id_signal_keep.append(frac_eios_id_nosignal)\n",
    "    pickle.dump(sample_eios_labels, open(sample_eios_labels_file, 'wb'))\n",
    "    del eios_labels, sample_eios_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized EIOS articles\n",
    "if tokenize_eios:\n",
    "    if not 'sample_eios_labels' in globals():\n",
    "        sample_eios_labels = pickle.load(open(sample_eios_labels_file, 'rb'))\n",
    "    write_log(log_path, 'Start tokenizing sample EIOS')\n",
    "    eios_tokens = get_eios(['id', 'tokens_simple_pp', 'tokens_full_pp'], \n",
    "                           sample_eios_labels,\n",
    "                           eios_data_type, read_eios_from_date, read_eios_to_date, \n",
    "                           eios_data_path, keep_boards, filter_word_length,\n",
    "                           filter_text_n_letters, True, None)\n",
    "    write_log(log_path, 'Tokenizing sample EIOS done')\n",
    "    pickle.dump(eios_tokens, open(eios_tokens_file, 'wb'))\n",
    "    del sample_eios_labels, eios_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relevant bigrams in both lists of tokenized texts\n",
    "if train_trigrams:\n",
    "    if not 'eios_tokens' in globals():   \n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "        \n",
    "    # Examples: \n",
    "    # trigram_simple_pp[bigram_simple_pp[['human','immunodeficiency','virus']]]\n",
    "    # > ['human_immunodeficiency_virus']\n",
    "    # trigram_simple_pp[bigram_simple_pp[['human','immunodeficiency','apple']]]\n",
    "    # > ['human_immunodeficiency', 'apple']\n",
    "    \n",
    "    # Full preprocessing\n",
    "    phrases_full_pp = Phrases([sentence for text in eios_tokens.tokens_full_pp \n",
    "                               for sentence in text])\n",
    "    bigram_full_pp = Phraser(phrases_full_pp)\n",
    "    phrases_full_pp_bi = Phrases([bigram_full_pp[sentence] \n",
    "                                  for text in eios_tokens.tokens_full_pp \n",
    "                                  for sentence in text])\n",
    "    trigram_full_pp = Phraser(phrases_full_pp_bi)\n",
    "    bigram_full_pp.save(bigram_full_pp_file)\n",
    "    trigram_full_pp.save(trigram_full_pp_file)\n",
    "                              \n",
    "    # Simple preprocessing\n",
    "    phrases_simple_pp = Phrases([sentence for text in eios_tokens.tokens_simple_pp \n",
    "                                 for sentence in text])\n",
    "    bigram_simple_pp = Phraser(phrases_simple_pp)\n",
    "    phrases_simple_pp_bi = Phrases([bigram_simple_pp[sentence] \n",
    "                                    for text in eios_tokens.tokens_simple_pp \n",
    "                                    for sentence in text])\n",
    "    trigram_simple_pp = Phraser(phrases_simple_pp_bi)\n",
    "    bigram_simple_pp.save(bigram_simple_pp_file)\n",
    "    trigram_simple_pp.save(trigram_simple_pp_file)\n",
    "    del (eios_tokens, phrases_full_pp, phrases_simple_pp, bigram_full_pp,\n",
    "         bigram_simple_pp, phrases_full_pp_bi, phrases_simple_pp_bi, trigram_full_pp,\n",
    "         trigram_simple_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization: Bag-Of-Words with tf-idf\n",
    "if compute_tfidf:\n",
    "    if not 'eios_tokens' in globals():   \n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    if not 'bigram_full_pp' in globals():\n",
    "        bigram_full_pp = Phrases().load(bigram_full_pp_file)\n",
    "        trigram_full_pp = Phrases().load(trigram_full_pp_file)\n",
    "\n",
    "    text_list_full_pp = []\n",
    "    for text in eios_tokens.tokens_full_pp:\n",
    "        sentence_conc = []\n",
    "        for sentence in text:\n",
    "            sentence_conc = sentence_conc + sentence\n",
    "        text_list_full_pp.append(trigram_full_pp[bigram_full_pp[sentence_conc]])\n",
    "                                 \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range_tfidf)\n",
    "    eios_tfidf = tfidf_vectorizer.fit_transform(\n",
    "        [' '.join(text) for text in text_list_full_pp])\n",
    "    eios_tfidf_dictionary = tfidf_vectorizer.vocabulary_\n",
    "    pickle.dump(eios_tfidf, open(eios_tfidf_file, 'wb'))\n",
    "    pickle.dump(eios_tfidf_dictionary,\n",
    "                open(eios_tfidf_dictionary_file, 'wb'))\n",
    "    del (bigram_full_pp, trigram_full_pp, eios_tokens, text_list_full_pp,\n",
    "         eios_tfidf, eios_tfidf_dictionary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization: Word2Vec\n",
    "if load_w2v_from_bin:\n",
    "    w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../data/embeddings/GoogleNews-vectors-negative300.bin', \n",
    "        binary=True, limit=limit_load_w2v)\n",
    "    w2v.init_sims(replace=True)\n",
    "    pickle.dump(w2v, open(w2v_pickle_file, 'wb'))\n",
    "\n",
    "if compute_w2v:\n",
    "    if not 'eios_tokens' in globals():   \n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    if not 'bigram_simple_pp' in globals():\n",
    "        bigram_simple_pp = Phrases().load(bigram_simple_pp_file)\n",
    "        trigram_simple_pp = Phrases().load(trigram_simple_pp_file)\n",
    "\n",
    "    write_log(log_path, 'Start concatenate sentences and build w2v bigrams')\n",
    "    text_list_simple_pp = []\n",
    "    for text in eios_tokens.tokens_simple_pp:\n",
    "        sentence_conc = []\n",
    "        for sentence in text:\n",
    "            sentence_conc = sentence_conc + sentence\n",
    "        text_list_simple_pp.append(trigram_simple_pp[bigram_simple_pp[sentence_conc]])\n",
    "    del eios_tokens, bigram_simple_pp, trigram_simple_pp\n",
    "    write_log(log_path, 'Concatenate sentences and build w2v bigrams done')\n",
    "    \n",
    "    write_log(log_path, 'Start loading w2v from pickle')\n",
    "    if not load_w2v_from_bin:\n",
    "        w2v = pickle.load(open(w2v_pickle_file, 'rb'))\n",
    "    write_log(log_path, 'Loading w2v from pickle done')\n",
    "    \n",
    "    # Give examples of how w2v works\n",
    "    w2v_examples_string = ''\n",
    "    for word in ['Ebola','HIV','influenza','H#N#']:\n",
    "        w2v_examples_string += ('> w2v.vectors_norm[w2v.vocab[\\''\n",
    "                                + word + '\\'].index]' + '\\n')\n",
    "        w2v_examples_string += (\n",
    "            '> [' \n",
    "            + ', '.join(\n",
    "                [str(coord) \n",
    "                 for coord in w2v.vectors_norm[w2v.vocab[word].index][0:10]])\n",
    "            + ' ... ]'\n",
    "        )\n",
    "        w2v_examples_string += '\\n\\n'\n",
    "        w2v_examples_string += '> w2v.most_similar(\\'' + word + '\\')' + '\\n'\n",
    "        w2v_examples_string += '> '+'\\n'.join(\n",
    "            [str(msw) for msw in w2v.most_similar(word)])\n",
    "        w2v_examples_string += '\\n\\n'\n",
    "    with open(w2v_examples_file,'w+') as out_file:\n",
    "        out_file.write(w2v_examples_string)\n",
    "    \n",
    "    write_log(log_path, 'Start w2v processing')        \n",
    "    eios_w2v = [word_embeddings_mean(w2v, text) for text in text_list_simple_pp]\n",
    "    write_log(log_path, 'w2v processing done')\n",
    "    pickle.dump(eios_w2v, open(eios_w2v_file, 'wb'))\n",
    "    del w2v, eios_w2v, text_list_simple_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty w2v\n",
    "if find_empty_w2v:\n",
    "    if not 'eios_labels' in globals():\n",
    "        eios_labels = pickle.load(open(eios_labels_file, 'rb'))\n",
    "    if not 'eios_w2v' in globals():\n",
    "        eios_w2v = pickle.load(open(eios_w2v_file, 'rb'))        \n",
    "    i_empty = [i for i in range(len(eios_w2v)) if eios_w2v[i] is None]\n",
    "    eios_labels_empty_w2v = eios_labels.loc[i_empty]\n",
    "    display(eios_labels_empty_w2v)\n",
    "    write_log(log_path, 'There are ' + str(eios_labels_empty_w2v.shape[0]) \n",
    "              + ' articles in the sample without embeddings, of them '\n",
    "              + str(sum(eios_labels_empty_w2v.signal)) + ' is/are signal/s')\n",
    "    del eios_labels, eios_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling\n",
    "if topic_modeling:\n",
    "    if not 'eios_tokens' in globals():\n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    if not 'eios_tfidf' in globals():\n",
    "        eios_tfidf = pickle.load(open(eios_tfidf_file, 'rb'))\n",
    "    if not 'eios_tfidf_dictionary' in globals():        \n",
    "        eios_tfidf_dictionary = pickle.load(open(eios_tfidf_dictionary_file, 'rb'))\n",
    "    \n",
    "    write_log(log_path, 'Start topic modeling')\n",
    "    gensim_dict_tfidf = dict((i, word) for word, i in eios_tfidf_dictionary.items())\n",
    "    topics = {}\n",
    "    topics_string = ''\n",
    "    for is_signal in [0,1]:\n",
    "        topics[is_signal] = LdaModel(\n",
    "            corpus=Sparse2Corpus(\n",
    "                eios_tfidf[[i for i in range(eios_tokens.shape[0])\n",
    "                            if eios_tokens.signal.iloc[i]==is_signal]]),\n",
    "            id2word=gensim_dict_tfidf, num_topics=n_topics, random_state=42, update_every=1,\n",
    "            chunksize=chunksize_topics, passes=10, alpha='auto', per_word_topics=True\n",
    "        )\n",
    "        topics_string += ('Topics for is_signal = '+str(is_signal)+'\\n'\n",
    "                          + '\\n'.join([str(top) for top in topics[is_signal].print_topics()])\n",
    "                          + '\\n')\n",
    "    with open(topics_text_file,'w+') as out_file:\n",
    "        out_file.write(topics_string)\n",
    "    \n",
    "    write_log(log_path, 'Topic modeling done')\n",
    "    pickle.dump(topics, open(topics_file,'wb'))\n",
    "    del eios_tokens, eios_tfidf, eios_tfidf_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "# N.B. It would be better to keep the punctuation and \"I\"\n",
    "if sentiment_analysis:\n",
    "    if not 'eios_tokens' in globals():\n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    \n",
    "    write_log(log_path, 'Start sentiment analysis')\n",
    "    sentiment = pd.DataFrame(columns=['polarity','subjectivity','signal'])\n",
    "    for i in eios_tokens.index:\n",
    "        text = eios_tokens.tokens_simple_pp.iloc[i]\n",
    "        signal = eios_tokens.signal.iloc[i]\n",
    "        sentence_conc = []\n",
    "        for sentence in text:\n",
    "            sentence_conc = sentence_conc + sentence\n",
    "        text_sentiment = TextBlob(' '.join(sentence_conc)).sentiment\n",
    "        sentiment = sentiment.append({\n",
    "            'polarity':text_sentiment.polarity,\n",
    "            'subjectivity':text_sentiment.subjectivity,\n",
    "            'signal':signal\n",
    "        }, ignore_index=True)\n",
    "    write_log(log_path, 'Sentiment analysis done')\n",
    "    pickle.dump(sentiment, open(sentiment_file,'wb'))\n",
    "    del eios_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment\n",
    "if plot_sentiment:\n",
    "    if not 'sentiment' in globals():\n",
    "        sentiment = pickle.load(open(sentiment_file, 'rb'))    \n",
    "    \n",
    "    sentiment_plot = (\n",
    "        alt.Chart(sentiment.loc[sentiment.signal==0])\n",
    "        .mark_circle(size=5,opacity=0.2).encode(\n",
    "            x='subjectivity:Q',\n",
    "            y='polarity:Q',\n",
    "            color='signal:N'\n",
    "        )\n",
    "        +\n",
    "        alt.Chart(sentiment.loc[sentiment.signal==1],\n",
    "                  title = 'sentiment')\n",
    "        .mark_circle().encode(\n",
    "            x='subjectivity:Q',\n",
    "            y='polarity:Q',\n",
    "            color='signal:N'\n",
    "        )\n",
    "    ).interactive()\n",
    "    sentiment_plot.save(sentiment_plot_file + '.html')\n",
    "    \n",
    "    # Box plots\n",
    "    for sent_type in ['polarity','subjectivity']:\n",
    "        sentiment_boxplot = (alt.Chart(sentiment)\n",
    "                             .mark_boxplot(outliers=True).encode(\n",
    "                                 x=sent_type+':Q',\n",
    "                                 y=alt.Y('signal:N', scale=alt.Scale(rangeStep=75))\n",
    "                             )).interactive()\n",
    "        sentiment_boxplot.save(sentiment_plot_file + '-box-' + sent_type + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE of vectorizations\n",
    "if perform_tsne:\n",
    "    if not 'eios_tokens' in globals():\n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    if not 'eios_tfidf' in globals():\n",
    "        eios_tfidf = pickle.load(open(eios_tfidf_file, 'rb'))\n",
    "    if not 'eios_w2v' in globals():\n",
    "        eios_w2v = pickle.load(open(eios_w2v_file, 'rb'))\n",
    "    \n",
    "    eios_vectorizations = {'tfidf': eios_tfidf, 'w2v': eios_w2v}\n",
    "    tsne_results = {}    \n",
    "    for vc_met in ['tfidf','w2v']:\n",
    "        data_set = eios_vectorizations[vc_met]\n",
    "        labels = eios_tokens.signal\n",
    "        if vc_met == 'w2v':\n",
    "            i_notempty = [i for i in range(len(data_set)) \n",
    "                          if data_set[i] is not None]\n",
    "            data_set = [data_set[i] for i in i_notempty]\n",
    "            labels = labels.iloc[i_notempty].reset_index(drop=True)\n",
    "        \n",
    "        # Reduce number of tf-idf dimensions first, so data set is manageable\n",
    "        # for t-SNE\n",
    "        write_log(log_path, 'Start dim reduction / ' + vc_met)\n",
    "        if vc_met == 'tfidf':\n",
    "            dim_reduction_model = TruncatedSVD(n_components=n_components_dim_reduction)\n",
    "            data_set = dim_reduction_model.fit_transform(data_set)\n",
    "            write_log(log_path, 'Dim reduction done / ' + vc_met)\n",
    "        \n",
    "        write_log(log_path, 'Start t-SNE / ' + vc_met)\n",
    "        tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "        tsne_results_fit = tsne.fit_transform(data_set)\n",
    "        write_log(log_path, 't-SNE done / ' + vc_met)    \n",
    "        tsne_results_df = pd.DataFrame({'dim1':tsne_results_fit[:,0],\n",
    "                                        'dim2':tsne_results_fit[:,1],'signal':labels})\n",
    "        tsne_results[vc_met] = tsne_results_df\n",
    "    del eios_vectorizations, eios_tokens, eios_tfidf, eios_w2v\n",
    "    pickle.dump(tsne_results, open(tsne_results_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE\n",
    "if plot_tsne:\n",
    "    if not 'tsne_results' in globals():\n",
    "        tsne_results = pickle.load(open(tsne_results_file, 'rb'))\n",
    "\n",
    "    for vc_met in ['tfidf','w2v']:\n",
    "        tsne_results_df = tsne_results[vc_met]\n",
    "\n",
    "        tsne_plot_nosignal = (alt.Chart(tsne_results_df.loc[tsne_results_df.signal==0])\n",
    "                              .mark_circle(size=5,opacity=0.2).encode(\n",
    "                                  x='dim1',\n",
    "                                  y='dim2',\n",
    "                                  color='signal:N'\n",
    "                              ))\n",
    "        tsne_plot_signal = (alt.Chart(tsne_results_df.loc[tsne_results_df.signal==1],\n",
    "                                      title = 'dimension reduction of ' + vc_met + ' (t-SNE)')\n",
    "                            .mark_circle().encode(\n",
    "                                x='dim1',\n",
    "                                y='dim2',\n",
    "                                color='signal:N'\n",
    "                            ))\n",
    "        (tsne_plot_nosignal + tsne_plot_signal).save(tsne_plot_files[vc_met] + '.html')\n",
    "\n",
    "    del tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training and test sets\n",
    "if build_train_test_sets:\n",
    "    if not 'eios_tokens' in globals():\n",
    "        eios_tokens = pickle.load(open(eios_tokens_file, 'rb'))\n",
    "    if not 'eios_tfidf' in globals():\n",
    "        eios_tfidf = pickle.load(open(eios_tfidf_file, 'rb'))\n",
    "    if not 'eios_w2v' in globals():\n",
    "        eios_w2v = pickle.load(open(eios_w2v_file, 'rb'))\n",
    "        \n",
    "    write_log(log_path, 'Start building train and test sets')\n",
    "    eios_vectorizations = {'tfidf':eios_tfidf, 'tfidf_dr':eios_tfidf,\n",
    "                           'w2v': eios_w2v}\n",
    "    train_set = {}\n",
    "    test_set = {}    \n",
    "    for vc_met in vectorization_methods:\n",
    "        data_set = eios_vectorizations[vc_met]\n",
    "        labels = eios_tokens.signal        \n",
    "        if vc_met == 'w2v':\n",
    "            # w2v: Remove empty data\n",
    "            i_notempty = [i for i in range(len(data_set)) \n",
    "                          if data_set[i] is not None]\n",
    "            data_set = [data_set[i] for i in i_notempty]\n",
    "            labels = labels.iloc[i_notempty].reset_index(drop=True)\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_set, labels, test_size=0.2, random_state=10)\n",
    "        if sum(y_test) == 0:\n",
    "            raise ValueError('No positive labels in ', vec, ' test set... '\n",
    "                             + 'Try another random_state.')\n",
    "        train_set[vc_met] = {}\n",
    "        test_set[vc_met] = {}\n",
    "        # Balance dataset...\n",
    "        for us_met in upsampling_methods:\n",
    "            train_set[vc_met][us_met] = {}\n",
    "            test_set[vc_met][us_met] = {}\n",
    "            write_log(log_path, 'Upsampling: ' + vc_met + ' / ' + us_met)\n",
    "            if us_met == 'no_us':\n",
    "                # ... not\n",
    "                X_train_us, y_train_us = X_train, y_train\n",
    "            elif us_met == 'duplicate':\n",
    "                # ... by duplicating signals\n",
    "                ros = RandomOverSampler(random_state=42)\n",
    "                X_train_us, y_train_us = ros.fit_resample(X_train, y_train)\n",
    "            elif us_met == 'adasyn':\n",
    "                # ... by generating synthetic signals\n",
    "                X_train_us, y_train_us = ADASYN().fit_resample(X_train, y_train)\n",
    "                \n",
    "            # tfidf_dr = tfidf with dimension reduction, after upsampling\n",
    "            # also called latent semantic analysis\n",
    "            if vc_met == 'tfidf_dr':\n",
    "                dim_reduction_model = (TruncatedSVD(n_components=n_components_dim_reduction,\n",
    "                                                    random_state=42)\n",
    "                                       .fit(X_train_us))\n",
    "                X_train_us_dr = dim_reduction_model.transform(X_train_us)\n",
    "                X_test_dr = dim_reduction_model.transform(X_test)\n",
    "            else:\n",
    "                X_train_us_dr, X_test_dr = X_train_us, X_test \n",
    "\n",
    "            # Standardize, or not\n",
    "            for st in standardizing:\n",
    "                if st == 'stand':\n",
    "                    write_log(log_path, 'Standardize: ' + vc_met + ' / ' + us_met)\n",
    "                    # No centering for sparse data\n",
    "                    with_mean = not vc_met == 'tfidf'\n",
    "                    scaler = preprocessing.StandardScaler(\n",
    "                        with_mean=with_mean).fit(X_train_us_dr)\n",
    "                    X_train_us_dr_st = scaler.transform(X_train_us_dr)\n",
    "                    X_test_us_dr_st = scaler.transform(X_test_dr)        \n",
    "                else:\n",
    "                    X_train_us_dr_st = X_train_us_dr\n",
    "                    X_test_us_dr_st = X_test_dr\n",
    "                train_set[vc_met][us_met][st] = {'X': X_train_us_dr_st, 'y': y_train_us}\n",
    "                test_set[vc_met][us_met][st] = {'X': X_test_us_dr_st, 'y': y_test}\n",
    "        \n",
    "    write_log(log_path, 'Building train and test sets done')    \n",
    "    del eios_tokens, eios_vectorizations, eios_tfidf, eios_w2v\n",
    "    pickle.dump(train_set, open(trainset_file, 'wb'))\n",
    "    pickle.dump(test_set, open(testset_file, 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: Train\n",
    "if train_classification_models:\n",
    "    if not 'train_set' in globals():\n",
    "        train_set = pickle.load(open(trainset_file, 'rb'))\n",
    "    \n",
    "    trained_models = {}\n",
    "    for cl_met in classification_methods:\n",
    "        trained_models[cl_met] = {}\n",
    "        for vc_met in vectorization_methods:\n",
    "            trained_models[cl_met][vc_met] = {}\n",
    "            for us_met in upsampling_methods:\n",
    "                trained_models[cl_met][vc_met][us_met] = {}\n",
    "                for st in standardizing:\n",
    "                    write_log(log_path, 'Start training ' + cl_met \n",
    "                              + ' / ' + vc_met + ' / ' + us_met\n",
    "                              + ' / ' + st)\n",
    "                    X_train = train_set[vc_met][us_met][st]['X']\n",
    "                    y_train = train_set[vc_met][us_met][st]['y']\n",
    "                    if cl_met == 'complement_naive_bayes':\n",
    "                        clf = ComplementNB()\n",
    "                    elif cl_met == 'logistic_regression':\n",
    "                        clf = LogisticRegression(solver='lbfgs', penalty='l2',\n",
    "                                                 max_iter=max_iter_lr,\n",
    "                                                 random_state=42)\n",
    "                    elif cl_met == 'random_forest':\n",
    "                        clf = RandomForestClassifier(n_estimators=randomforest_n_estimators)\n",
    "                    elif cl_met == 'multilayer_perceptron':\n",
    "                        clf = MLPClassifier(hidden_layer_sizes=mlp_lsize,\n",
    "                                            max_iter=max_iter_mlp,\n",
    "                                            shuffle=True, random_state=42)                        \n",
    "                    elif cl_met == 'svm_rbf':\n",
    "                        clf = SVC(kernel='rbf', gamma='scale', random_state=42, \n",
    "                                  probability=True)\n",
    "                    else:\n",
    "                        raise ValueError('Unknown classification method ' + cl_met) \n",
    "                \n",
    "                    if ( (cl_met=='complement_naive_bayes') \n",
    "                          & ((vc_met=='w2v') | (vc_met=='tfidf_dr')) ):\n",
    "                        # Complement Naive Bayes works only with positive features, here:\n",
    "                        # only 'tfdif', both standardized (it's not centered) and not, i.e.\n",
    "                        # exclude 'tfidf_dr' and 'w2v'.\n",
    "                        trained_models[cl_met][vc_met][us_met][st] = None\n",
    "                    else:     \n",
    "                        clf.fit(X_train, y_train)\n",
    "                        trained_models[cl_met][vc_met][us_met][st] = clf\n",
    "                        \n",
    "    write_log(log_path, 'Training done')\n",
    "    del train_set\n",
    "    pickle.dump(trained_models, open(trained_models_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: Test, i.e. compute scores\n",
    "if compute_scores:\n",
    "    if not 'trained_models' in globals():\n",
    "        trained_models = pickle.load(open(trained_models_file, 'rb'))\n",
    "    if not 'test_set' in globals():   \n",
    "        test_set = pickle.load(open(testset_file, 'rb'))\n",
    "    \n",
    "    ## DEBUG:\n",
    "#     cl_met, vc_met, us_met = 'logistic_regression', 'w2v', 'adasyn'\n",
    "#     cl_met, vc_met, us_met, st = 'complement_naive_bayes', 'tfidf', 'adasyn', 'no_st'\n",
    "    scores_thresholds_init = pd.DataFrame(columns = ['classifier','vectorization','upsampling',\n",
    "                                                     'standardize','score_type','threshold',\n",
    "                                                     'score_value'])\n",
    "    scores_thresholds_cm_max_init = pd.DataFrame(columns = ['classifier','vectorization',\n",
    "                                                            'upsampling','standardize',\n",
    "                                                            'score_type','threshold',\n",
    "                                                            'score_value','confusion_matrix'])\n",
    "    scores_fixed_recall_init = pd.DataFrame(columns = ['classifier','vectorization',\n",
    "                                                       'upsampling','standardize',\n",
    "                                                       'score_type','threshold',\n",
    "                                                       'score_value','confusion_matrix'])\n",
    "    proba_signal_df_all = pd.DataFrame(columns = ['classifier','vectorization',\n",
    "                                                   'upsampling','standardize',\n",
    "                                                   'probability','signal'])\n",
    "    \n",
    "    scores_thresholds_all = scores_thresholds_init\n",
    "    scores_thresholds_cm_max_all = scores_thresholds_cm_max_init\n",
    "    scores_fixed_recall_all = scores_fixed_recall_init\n",
    "    \n",
    "    write_log(log_path, 'Start testing')\n",
    "    for cl_met in classification_methods:\n",
    "        for vc_met in vectorization_methods:\n",
    "            for us_met in upsampling_methods:\n",
    "                for st in standardizing:\n",
    "                    clf = trained_models[cl_met][vc_met][us_met][st]\n",
    "                    if clf is not None:\n",
    "                        \n",
    "                        write_log(log_path, 'Start scoring ' + cl_met \n",
    "                                  + ' / ' + vc_met + ' / ' + us_met\n",
    "                                  + ' / ' + st)\n",
    "                        \n",
    "                        scores_thresholds = scores_thresholds_init\n",
    "                        scores_thresholds_cm_max = scores_thresholds_cm_max_init\n",
    "                        \n",
    "                        X_test = test_set[vc_met][us_met][st]['X']\n",
    "                        y_test = test_set[vc_met][us_met][st]['y']\n",
    "                        probabilities = clf.predict_proba(X_test)[:,1]\n",
    "                        \n",
    "                        # Set the threholds list\n",
    "                        th_list = [th/n_thresholds for th in range(n_thresholds+1)]\n",
    "                        # Use roc_curve() and precision_recall_curve() to refine them,\n",
    "                        # to catch rapid variations in the scores\n",
    "                        th_list = (th_list\n",
    "                                   + list(roc_curve(y_test, probabilities)[2])\n",
    "                                   + list(precision_recall_curve(y_test, probabilities)[2]))\n",
    "                        # Remove artifacts of threshold > 1\n",
    "                        th_list = [th for th in th_list if th <= 1]\n",
    "                        th_list = np.sort(np.unique(th_list))\n",
    "                        \n",
    "                        # Scores for varying thresholds, find optima\n",
    "                        for sc in scores_list:\n",
    "\n",
    "                            if sc == 'mcc':\n",
    "                                score_max = -1\n",
    "                            else:\n",
    "                                score_max = 0\n",
    "                            th_max = None\n",
    "                            y_pred_th_max = None\n",
    "\n",
    "                            for th in th_list:\n",
    "                                y_pred_th = [1 if y >= th else 0 for y in probabilities]\n",
    "                                if sc == 'accuracy':\n",
    "                                    score = accuracy_score(y_test, y_pred_th)\n",
    "                                elif sc == 'precision':\n",
    "                                    score = precision_score(y_test, y_pred_th)\n",
    "                                elif sc == 'recall':\n",
    "                                    score = recall_score(y_test, y_pred_th, pos_label=1)\n",
    "                                elif sc == 'specificity':\n",
    "                                    score = recall_score(y_test, y_pred_th, pos_label=0)\n",
    "                                elif sc == 'f1':   \n",
    "                                    score = f1_score(y_test, y_pred_th)\n",
    "                                elif sc == 'mcc':\n",
    "                                    score = matthews_corrcoef(y_test, y_pred_th)\n",
    "                                elif sc == 'ba':\n",
    "                                    score = balanced_accuracy_score(y_test, y_pred_th)\n",
    "                                elif sc == 'geom_mean':\n",
    "                                    score = geometric_mean_score(y_test, y_pred_th)\n",
    "                                elif sc == 'iba_gm':\n",
    "                                    iba_gm = make_index_balanced_accuracy(\n",
    "                                        alpha=alpha_iba, squared=True)(geometric_mean_score)\n",
    "                                    score = iba_gm(y_test, y_pred_th)\n",
    "                                else:\n",
    "                                    raise ValueError('Unknown score ' + sc + '!\\n')\n",
    "\n",
    "                                if score >= score_max:\n",
    "                                    score_max = score\n",
    "                                    th_max = th\n",
    "                                    y_pred_th_max = y_pred_th\n",
    "\n",
    "                                scores_thresholds = scores_thresholds.append({\n",
    "                                    'classifier':cl_met,'vectorization':vc_met,\n",
    "                                    'upsampling':us_met,'standardize':st,\n",
    "                                    'score_type':sc,'threshold':th,\n",
    "                                    'score_value':score\n",
    "                                }, ignore_index=True)\n",
    "\n",
    "                            cm_max_df = pd.DataFrame(confusion_matrix(y_test, y_pred_th_max),\n",
    "                                                     columns=['pred0','pred1'],\n",
    "                                                     index=['label0','label1'])\n",
    "                            scores_thresholds_cm_max = scores_thresholds_cm_max.append({\n",
    "                                'classifier':cl_met,'vectorization':vc_met,\n",
    "                                'upsampling':us_met,'standardize':st,\n",
    "                                'score_type':sc,'threshold':th_max,\n",
    "                                'score_value':score_max,\n",
    "                                'confusion_matrix':cm_max_df\n",
    "                            }, ignore_index=True)\n",
    "\n",
    "                        auc = roc_auc_score(y_test, probabilities)\n",
    "                        scores_thresholds_cm_max = scores_thresholds_cm_max.append({\n",
    "                            'classifier':cl_met,'vectorization':vc_met,\n",
    "                            'upsampling':us_met,'standardize':st,\n",
    "                            'score_type':'auc','threshold':None,\n",
    "                            'score_value':auc,\n",
    "                            'confusion_matrix':None\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "                        proba_signal_df = pd.DataFrame({\n",
    "                            'classifier':cl_met,'vectorization':vc_met,\n",
    "                            'upsampling':us_met,'standardize':st,\n",
    "                            'probability': probabilities,\n",
    "                            'signal': y_test\n",
    "                        })\n",
    "                        rel_p_gap = (\n",
    "                            2*(np.mean(proba_signal_df[proba_signal_df.signal==1].probability)\n",
    "                               - np.mean(proba_signal_df[proba_signal_df.signal==0].probability))\n",
    "                            / (np.std(proba_signal_df[proba_signal_df.signal==1].probability)\n",
    "                               + np.std(proba_signal_df[proba_signal_df.signal==0].probability))\n",
    "                        )    \n",
    "                        scores_thresholds_cm_max = scores_thresholds_cm_max.append({\n",
    "                            'classifier':cl_met,'vectorization':vc_met,\n",
    "                            'upsampling':us_met,'standardize':st,\n",
    "                            'score_type':'rel_p_gap','threshold':None,\n",
    "                            'score_value':rel_p_gap,\n",
    "                            'confusion_matrix':None\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "                        # Find scores for threshold such that recall=recall_target\n",
    "                        recall_above_target = [recall \n",
    "                            for recall in scores_thresholds[\n",
    "                                    scores_thresholds.score_type=='recall'\n",
    "                                ].score_value\n",
    "                            if recall >= recall_target]\n",
    "                        # If there are many thresholds found, take the largest, that would\n",
    "                        # lead to a smaller recall (presumably closer to target) and e.g. \n",
    "                        # higher precision or specificity\n",
    "                        th_fixed_recall = max(scores_thresholds.loc[\n",
    "                            [(scores_thresholds.score_type.iloc[i]=='recall')\n",
    "                             & (scores_thresholds.score_value.iloc[i]==min(recall_above_target))\n",
    "                             for i in range(scores_thresholds.shape[0])]\n",
    "                        ].threshold)\n",
    "                        scores_fixed_recall = scores_thresholds.copy().loc[\n",
    "                            [i for i in scores_thresholds.index\n",
    "                             if scores_thresholds.threshold.iloc[i]==th_fixed_recall]\n",
    "                        ]\n",
    "                        y_pred_fixed_recall = [1 if y >= float(th_fixed_recall) else 0\n",
    "                                               for y in probabilities]\n",
    "                        cm_fr_df = pd.DataFrame(confusion_matrix(y_test, y_pred_fixed_recall),\n",
    "                                                columns=['pred0','pred1'],\n",
    "                                                index=['label0','label1'])\n",
    "                        scores_fixed_recall['confusion_matrix'] = [cm_fr_df \n",
    "                                                                   for i in scores_fixed_recall.index]\n",
    "                        \n",
    "                        # Append data frames\n",
    "                        scores_thresholds_all = scores_thresholds_all.append(\n",
    "                            scores_thresholds, ignore_index=True\n",
    "                        )\n",
    "                        scores_thresholds_cm_max_all = (\n",
    "                            scores_thresholds_cm_max_all.append(\n",
    "                                scores_thresholds_cm_max, ignore_index=True)\n",
    "                        )\n",
    "                        scores_fixed_recall_all = scores_fixed_recall_all.append(\n",
    "                            scores_fixed_recall, ignore_index=True\n",
    "                        )\n",
    "                        proba_signal_df_all = proba_signal_df_all.append(\n",
    "                            proba_signal_df,  ignore_index=True\n",
    "                        )\n",
    "\n",
    "    # Save scores\n",
    "    scores_dict = {'scores_thresholds':scores_thresholds_all,\n",
    "                   'scores_max':scores_thresholds_cm_max_all,\n",
    "                   'scores_recall':scores_fixed_recall_all,\n",
    "                   'proba_signal':proba_signal_df_all}\n",
    "    pickle.dump(scores_dict, open(scores_file,'wb'))\n",
    "    \n",
    "    write_log(log_path, 'testing done')\n",
    "    del (trained_models, test_set, scores_dict, scores_thresholds_all,\n",
    "         scores_thresholds_cm_max_all, scores_fixed_recall_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghozzis\\AppData\\Local\\Continuum\\anaconda3\\envs\\eios_anomaly\\lib\\site-packages\\altair\\utils\\core.py:90: UserWarning: I don't know how to infer vegalite type from 'empty'.  Defaulting to nominal.\n",
      "  \"Defaulting to nominal.\".format(typ))\n"
     ]
    }
   ],
   "source": [
    "# Plot and write classification scores\n",
    "if plot_write_scores:\n",
    "    if not 'scores_dict' in globals():\n",
    "        scores_dict = pickle.load(open(scores_file, 'rb'))\n",
    "        scores_thresholds_all = scores_dict['scores_thresholds']\n",
    "        scores_thresholds_cm_max_all = scores_dict['scores_max']\n",
    "        scores_fixed_recall_all = scores_dict['scores_recall']        \n",
    "        proba_signal_df_all = scores_dict['proba_signal']\n",
    "    \n",
    "    ## DEBUG:\n",
    "#     cl_met, vc_met, us_met, st = 'complement_naive_bayes', 'tfidf', 'adasyn', 'no_st'   \n",
    "#     cl_met, vc_met, us_met, st = 'logistic_regression', 'tfidf_dr', 'duplicate', 'no_st'\n",
    "    \n",
    "    for cl_met in classification_methods:\n",
    "        for vc_met in vectorization_methods:\n",
    "            for us_met in upsampling_methods:\n",
    "                for st in standardizing:\n",
    "                    if (not ((cl_met=='complement_naive_bayes') \n",
    "                          & ((vc_met=='w2v') | (vc_met=='tfidf_dr')))):\n",
    "                        \n",
    "                        scores_thresholds = scores_thresholds_all.loc[\n",
    "                            (scores_thresholds_all.classifier==cl_met)\n",
    "                            & (scores_thresholds_all.vectorization==vc_met)\n",
    "                            & (scores_thresholds_all.upsampling==us_met)\n",
    "                            & (scores_thresholds_all.standardize==st)\n",
    "                        ]\n",
    "                        scores_thresholds_cm_max = scores_thresholds_cm_max_all.loc[\n",
    "                            (scores_thresholds_cm_max_all.classifier==cl_met)\n",
    "                            & (scores_thresholds_cm_max_all.vectorization==vc_met)\n",
    "                            & (scores_thresholds_cm_max_all.upsampling==us_met)\n",
    "                            & (scores_thresholds_cm_max_all.standardize==st)\n",
    "                        ]\n",
    "                        scores_fixed_recall = scores_fixed_recall_all.loc[\n",
    "                            (scores_fixed_recall_all.classifier==cl_met)\n",
    "                            & (scores_fixed_recall_all.vectorization==vc_met)\n",
    "                            & (scores_fixed_recall_all.upsampling==us_met)\n",
    "                            & (scores_fixed_recall_all.standardize==st)\n",
    "                        ]\n",
    "                        proba_signal_df = proba_signal_df_all.loc[\n",
    "                            (proba_signal_df_all.classifier==cl_met)\n",
    "                            & (proba_signal_df_all.vectorization==vc_met)\n",
    "                            & (proba_signal_df_all.upsampling==us_met)\n",
    "                            & (proba_signal_df_all.standardize==st)\n",
    "                        ]\n",
    "                        th_list = [th \n",
    "                                  for th in scores_thresholds.loc[\n",
    "                                      scores_thresholds.score_type=='recall'].threshold]\n",
    "\n",
    "                        approach_dir = (output_dir + '/scores_individual_approaches/'\n",
    "                                        + cl_met + '-' + vec_tag[vc_met] + '-'\n",
    "                                        + us_met + '-' + st \n",
    "                                        + '_' + str(nosignal_sample_frac) + '_'\n",
    "                                        + str(nosignal_sample_seed) + '-' \n",
    "                                        + str(limit_load_w2v) + '-' \n",
    "                                        + file_name_date(read_eios_from_date, read_eios_to_date))\n",
    "                        if not os.path.exists(approach_dir):\n",
    "                            os.makedirs(approach_dir)                \n",
    "\n",
    "                        # ROC curve\n",
    "                        fpr = [1-spec \n",
    "                               for spec in scores_thresholds.loc[\n",
    "                                   scores_thresholds.score_type=='specificity'].score_value]\n",
    "                        recall = [rec \n",
    "                                  for rec in scores_thresholds.loc[\n",
    "                                   scores_thresholds.score_type=='recall'].score_value]\n",
    "                        roc_curve_df = pd.DataFrame({\n",
    "                            'fpr':fpr,\n",
    "                            'recall':recall,\n",
    "                            'threshold':th_list})\n",
    "                        roc_curve_plot =( \n",
    "                            alt.Chart(roc_curve_df).mark_line().encode(\n",
    "                                x=alt.X('fpr:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                y=alt.Y('recall:Q', scale=alt.Scale(domain=(0,1)))\n",
    "                            )\n",
    "                            +\n",
    "                             alt.Chart(roc_curve_df).mark_circle(size=20,opacity=0.2).encode(\n",
    "                                x=alt.X('fpr:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                y=alt.Y('recall:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                tooltip=['fpr','recall','threshold']\n",
    "                            ) \n",
    "                        ).interactive()\n",
    "                        roc_curve_plot.save(approach_dir + '/roc_curve.html')\n",
    "\n",
    "                        # Precision-Recall curve\n",
    "                        precision = [prec \n",
    "                                     for prec in scores_thresholds.loc[\n",
    "                                         scores_thresholds.score_type=='precision'].score_value]\n",
    "                        pr_curve_df = pd.DataFrame({\n",
    "                            'precision':precision,\n",
    "                            'recall':recall,\n",
    "                            'threshold':th_list})\n",
    "                        pr_curve_plot = (\n",
    "                            alt.Chart(pr_curve_df).mark_line().encode(\n",
    "                                x=alt.X('recall:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                y=alt.Y('precision:Q', scale=alt.Scale(domain=(0,1)))\n",
    "                            ) \n",
    "                            +\n",
    "                            alt.Chart(pr_curve_df).mark_circle(size=20,opacity=0.2).encode(\n",
    "                                x=alt.X('recall:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                y=alt.Y('precision:Q', scale=alt.Scale(domain=(0,1))),\n",
    "                                tooltip=['recall','precision','threshold']\n",
    "                            )\n",
    "                        ).interactive()\n",
    "                        pr_curve_plot.save(approach_dir + '/pr_curve.html')\n",
    "                        \n",
    "                        # All scores vs. threshold\n",
    "                        scores_thresholds_plot = (\n",
    "                            alt.Chart(scores_thresholds).mark_line().encode(\n",
    "                                x='threshold:Q',\n",
    "                                y='score_value:Q',\n",
    "                                color='score_type:N'\n",
    "                            )\n",
    "                            + \n",
    "                            alt.Chart(scores_thresholds).mark_circle(size=20,opacity=0.2).encode(\n",
    "                                x='threshold:Q',\n",
    "                                y='score_value:Q',\n",
    "                                color='score_type:N',\n",
    "                                tooltip=['score_type','score_value','threshold']\n",
    "                            ) \n",
    "                        ).interactive()\n",
    "                        scores_thresholds_plot.save(approach_dir + '/scores_thresholds.html')\n",
    "                        scores_thresholds_plot_log = (\n",
    "                            alt.Chart(scores_thresholds.loc[scores_thresholds.threshold > 0])\n",
    "                            .mark_line().encode(\n",
    "                                x=alt.X('threshold:Q',scale=alt.Scale(type='log')),\n",
    "                                y='score_value:Q',\n",
    "                                color='score_type:N'\n",
    "                            )\n",
    "                            + \n",
    "                            alt.Chart(scores_thresholds.loc[scores_thresholds.threshold > 0])\n",
    "                            .mark_circle(size=20,opacity=0.2)\n",
    "                            .encode(\n",
    "                                x=alt.X('threshold:Q',scale=alt.Scale(type='log')),\n",
    "                                y='score_value:Q',\n",
    "                                color='score_type:N',\n",
    "                                tooltip=['score_type','score_value','threshold']\n",
    "                            ) \n",
    "                        ).interactive()\n",
    "                        scores_thresholds_plot_log.save(\n",
    "                            approach_dir + '/scores_thresholds_log.html')\n",
    "\n",
    "                        # Probability vs. signal\n",
    "                        proba_signal_plot = (alt.Chart(proba_signal_df)\n",
    "                                              .mark_boxplot(outliers=True).encode(\n",
    "                                                  x='probability:Q',\n",
    "                                                  y=alt.Y('signal:N',\n",
    "                                                          scale=alt.Scale(rangeStep=75))\n",
    "                                              )).interactive()\n",
    "                        proba_signal_plot.save(approach_dir + '/proba_signal.html')\n",
    "                        proba_signal_plot_log = (alt.Chart(proba_signal_df)\n",
    "                                                  .mark_boxplot(outliers=True).encode(\n",
    "                                                      x=alt.X('probability:Q',\n",
    "                                                              scale=alt.Scale(type='log')),\n",
    "                                                      y=alt.Y('signal:N',\n",
    "                                                          scale=alt.Scale(rangeStep=75))\n",
    "                                                  )).interactive()\n",
    "                        proba_signal_plot_log.save(approach_dir + '/proba_signal_log.html')\n",
    "\n",
    "                        # Scores and confusion matrix at fixed recall\n",
    "                        scores_fixed_recall_print = scores_fixed_recall.copy()\n",
    "                        scores_fixed_recall_print.confusion_matrix = [\n",
    "                            None if cmat is None \n",
    "                            else ' / '.join([rn+cn+' '+str(cmat.loc[rn,cn])\n",
    "                                             for rn in cmat.index for cn in cmat.columns])\n",
    "                            for cmat in scores_fixed_recall.confusion_matrix\n",
    "                        ]\n",
    "                        with open(approach_dir+'/scores_fixed_recall.txt','w+') as out_file:\n",
    "                            out_file.write(scores_fixed_recall_print.to_string())\n",
    "\n",
    "                        # Max scores, AUC, relative prob gap\n",
    "                        scores_thresholds_cm_max_print = scores_thresholds_cm_max.copy()\n",
    "                        scores_thresholds_cm_max_print.confusion_matrix = [\n",
    "                            None if cmat is None \n",
    "                            else ' / '.join([rn+cn+' '+str(cmat.loc[rn,cn])\n",
    "                                             for rn in cmat.index for cn in cmat.columns])\n",
    "                            for cmat in scores_thresholds_cm_max.confusion_matrix\n",
    "                        ]\n",
    "                        with open(approach_dir+'/scores_max_auc_rpg.txt','w+') as out_file:\n",
    "                            out_file.write(scores_thresholds_cm_max_print.to_string())                 \n",
    "    \n",
    "    del (scores_dict, scores_thresholds_all,\n",
    "         scores_thresholds_cm_max_all, scores_fixed_recall_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "if overview_ouput:\n",
    "    if not 'scores_dict' in globals():\n",
    "        scores_dict = pickle.load(open(scores_file, 'rb'))\n",
    "        scores_thresholds_cm_max_all = scores_dict['scores_max']\n",
    "        scores_fixed_recall_all = scores_dict['scores_recall']\n",
    "\n",
    "    overview_dir = (output_dir + '/scores_overview/'\n",
    "                    + str(nosignal_sample_frac) + '_'\n",
    "                    + str(nosignal_sample_seed) + '-' \n",
    "                    + str(limit_load_w2v) + '-' \n",
    "                    + file_name_date(read_eios_from_date, read_eios_to_date))\n",
    "    if not os.path.exists(overview_dir):\n",
    "        os.makedirs(overview_dir)\n",
    "\n",
    "    best_approaches = {'fixed_recall':pd.DataFrame(columns=['score_type','score_value',\n",
    "                                                            'approach','confusion_matrix']),\n",
    "                       'max_auc_rpg': pd.DataFrame(columns=['score_type','score_value',\n",
    "                                                            'approach','confusion_matrix'])}\n",
    "    for sc in scores_list+['auc','rel_p_gap']:\n",
    "        if sc == 'mcc':\n",
    "            min_score = -1\n",
    "        else:\n",
    "            min_score = 0\n",
    "\n",
    "        for overview_type in ['fixed_recall','max_auc_rpg']:\n",
    "            if overview_type == 'fixed_recall':\n",
    "                # Scores at fixed recall\n",
    "                scores_df = scores_fixed_recall_all.copy()\n",
    "                rank_plot_title = 'approaches ranked by ' + sc + ' (fixed recall)'\n",
    "            elif overview_type == 'max_auc_rpg':\n",
    "                # Max scores, AUC, relative prob gap\n",
    "                scores_df = scores_thresholds_cm_max_all.copy()\n",
    "                if sc in ['auc','rel_p_gap']:\n",
    "                    rank_plot_title = 'approaches ranked by ' + sc\n",
    "                else:\n",
    "                    rank_plot_title = 'approaches ranked by best ' + sc\n",
    "\n",
    "            # Exclude trivial max scores\n",
    "            if (((overview_type == 'fixed_recall')\n",
    "                 & (not sc in ['recall','auc','rel_p_gap']))\n",
    "                or ((overview_type == 'max_auc_rpg')\n",
    "                    & (not sc in ['recall','specificity']))):\n",
    "\n",
    "                # Plot ranking of approaches\n",
    "                rank_plot_df = (scores_df[scores_df.score_type==sc]\n",
    "                                .sort_values(by='score_value',ascending=False))\n",
    "                rank_plot_df['approach'] = [\n",
    "                    '-'.join([''.join([lab1[0] for lab1 in lab.split('_')]) \n",
    "                              for lab in rank_plot_df.loc[i,\n",
    "                                  ['classifier','vectorization','upsampling',\n",
    "                                   'standardize']]])                    \n",
    "                    for i in rank_plot_df.index\n",
    "                ]\n",
    "                rank_plot_df['approach_full'] = [\n",
    "                    '-'.join(rank_plot_df.loc[i,['classifier','vectorization',\n",
    "                                                 'upsampling','standardize']]) \n",
    "                    for i in rank_plot_df.index\n",
    "                ]                \n",
    "                if sc == 'rel_p_gap':\n",
    "                    rank_plot_df = rank_plot_df[~np.isnan(rank_plot_df.score_value)]\n",
    "                else:    \n",
    "                    rank_plot_df = rank_plot_df[rank_plot_df.score_value > min_score]\n",
    "\n",
    "                rank_plot = (\n",
    "                    alt.Chart(rank_plot_df.loc[:,['approach','score_value']],\n",
    "                              title = rank_plot_title)\n",
    "                    .mark_bar()\n",
    "                    .encode(\n",
    "                        x = alt.X('approach:N', sort = None),\n",
    "                        y = alt.Y('score_value:Q', axis=alt.Axis(title=sc)),\n",
    "                        tooltip = ['approach', 'score_value']\n",
    "                    ).interactive()\n",
    "                )\n",
    "                rank_plot.save(overview_dir+'/ranked_approaches_'+overview_type+'_'+sc+'.html')\n",
    "                rank_plot_top10 = (\n",
    "                    alt.Chart(rank_plot_df.loc[:,['approach','score_value']].iloc[0:10,:],\n",
    "                              title = 'top 10 ' + rank_plot_title)\n",
    "                    .mark_bar()\n",
    "                    .encode(\n",
    "                        x = alt.X('approach:N', sort = None, scale=alt.Scale(rangeStep=50)),\n",
    "                        y = alt.Y('score_value:Q', axis=alt.Axis(title=sc)),\n",
    "                        tooltip = ['approach', 'score_value']\n",
    "                    ).interactive()\n",
    "                )\n",
    "                rank_plot_top10.save(overview_dir+'/ranked_approaches_'\n",
    "                                     +overview_type+'_'+sc+'_top10.html')\n",
    "                \n",
    "                # Get best approaches\n",
    "                best_approaches[overview_type] = best_approaches[overview_type].append({\n",
    "                    'score_type':sc,\n",
    "                    'score_value':rank_plot_df.score_value.iloc[0],\n",
    "                    'approach':rank_plot_df.approach_full.iloc[0],\n",
    "                    'confusion_matrix':rank_plot_df.confusion_matrix.iloc[0]\n",
    "                }, ignore_index=True)\n",
    "\n",
    "    # Print best approaches\n",
    "    for overview_type in ['fixed_recall','max_auc_rpg']:\n",
    "        best_approaches_print = best_approaches[overview_type].copy()\n",
    "        best_approaches_print.confusion_matrix = [\n",
    "            None if cmat is None\n",
    "            else ' / '.join([rn+cn+' '+str(cmat.loc[rn,cn])\n",
    "                             for rn in cmat.index for cn in cmat.columns])\n",
    "            for cmat in best_approaches[overview_type].confusion_matrix\n",
    "        ]\n",
    "        with open(overview_dir+'/best_approaches_'+overview_type+'.txt','w+') as out_file:\n",
    "            out_file.write(best_approaches_print.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eios_anomaly)",
   "language": "python",
   "name": "eios_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}